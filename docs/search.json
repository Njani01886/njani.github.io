[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nayan Jani’s Website",
    "section": "",
    "text": "I am Nayan Jani, a dedicated graduate student with a Bachelor’s degree in Data Science from the University of Rhode Island. Currently, I am pursuing a Master’s degree in Data Analytics and Computational Social Science (DACSS) at the University of Massachusetts Amherst. Throughout my academic journey, I’ve developed a solid foundation in data cleaning and processing, advanced quantitative methods, machine learning, and data visualization, and I am eager to apply my knowledge to real-world challenges. To view my code for some of my projects, go to the sidebar on the left. To view my completed projects, click on the about me tab.\n \n  \n   \n  \n    \n     Email\n  \n  \n    \n     Github\n  \n  \n    \n     Linkedin"
  },
  {
    "objectID": "BlogPost6_NayanJani.html",
    "href": "BlogPost6_NayanJani.html",
    "title": "Assessing Sentiment Surrounding the 2022 World Cup",
    "section": "",
    "text": "Introduction\nThe FIFA 2022 World Cup has captivated almost everyone’s attention this year. However, most of that attention has been focused on human rights violations that are present. The Host nation Qatar has been under pressure for these violations because of their treatment of foreigners in their country and their failure to be inclusive. Migrant workers that have helped build stadiums have been mistreated, underpaid, overworked and even killed leading up to the World Cup. The Host nation also has disallowed for the LGBTQ community to represent themselves because of the Host nations beliefs. Soccer Fans from both cultures (Host nation vs Foreigners) have argued over what values to respect on the global level.\n\n\nObjectives\nFind the overall Sentiment of the comments (Positive and Negative, Other emotions)\nWhat is the main focus of discussion in the comments? What topic is most important to the people in the comments?\nBased on the most important topics and the sentiment of those comments, are those comments classified correctly positive or negative? If yes, what are the comments POV? (western culture vs middle east culture) Are those comments “socially correct”? (logical/acceptable POV vs Stereotyped/Stigmatized POV)\n\n\nRead in Data\nUsing Youtube API and Python, I was able to extract comments from nine videos covering the world cup in Qatar. The sources of the videos include BBC, Sky Sports News, France 21 and independent content creators. I chose videos based on the the amount of views. The comments I have scraped are the top 100 most relevant comments and the top 100 most recent comments from each video. The total number of comments I scraped was 1,391 . As a part of pre-processing my data, I removed all comments with less than 3 tokens in them using the tidyverse. I also removed any symbols, punctuation URLs , numbers and stopwords from my data . I tokenized my data using three different libraries, quanteda, tidytext and text2vec.\n\n\nCode\ndf_bbc<- read_csv(\"_data/comments_bbc.csv\")\n\ndf_bbc<- df_bbc%>% \n  rename(text = \"i\")\n\ndf_bbc<- df_bbc %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbc <- df_bbc %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbc <- corpus(df_bbc)\ncorpus_bbc_summary <- summary(corpus_bbc)\n\ncorpus_bbc_summary$video <- \"BBC\"\ndocvars(corpus_bbc) <- corpus_bbc_summary\n\ndf_q<- read_csv(\"_data/comments_q.csv\")\n\ndf_q<- df_q %>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_q<- df_q %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\n\ndf_q <- df_q %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_q <- corpus(df_q)\ncorpus_q_summary <- summary(corpus_q)\ncorpus_q_summary$video <- \"Maqwell\"\ndocvars(corpus_q) <- corpus_q_summary\n\ndf_qRev<- read_csv(\"_data/comments_qRev.csv\")\n\ndf_qRev <- df_qRev%>% \n  rename(text = \"I’ll try to get the next video essay out in less than a month lol\")\n\ndf_qRev<- df_qRev %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_qRev <- df_qRev %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_qRev <- corpus(df_qRev)\ncorpus_qRev_summary <- summary(corpus_qRev)\ncorpus_qRev_summary$video <- \"MaqwellRev\"\ndocvars(corpus_qRev) <- corpus_qRev_summary\n\ndf_sky<- read_csv(\"_data/comments_sky.csv\")\n\ndf_sky<- df_sky%>% \n  rename(text = \"i\")\n\ndf_sky<- df_sky %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_sky <- df_sky %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_sky <- corpus(df_sky)\ncorpus_sky_summary <- summary(corpus_sky)\ncorpus_sky_summary$video <- \"sky\"\ndocvars(corpus_sky) <- corpus_sky_summary\n\n\ndf_bbcQ <- read_csv(\"_data/comments_bbcQ.csv\")\n\n\ndf_bbcQ<- df_bbcQ%>% \n  rename(text = \"i\")\n\ndf_bbcQ<- df_bbcQ %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbcQ <- df_bbcQ %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbcQ<- corpus(df_bbcQ)\ncorpus_bbcQ_summary <- summary(corpus_bbcQ)\ncorpus_bbcQ_summary$video <- \"BBC\"\n\ndf_bbcOL <- read_csv(\"_data/comments_bbcOL.csv\")\n\n\ndf_bbcOL<- df_bbcOL%>% \n  rename(text = \"i\")\n\ndf_bbcOL<- df_bbcOL %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_bbcOL <- df_bbcOL %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_bbcOL<- corpus(df_bbcOL)\ncorpus_bbcOL_summary <- summary(corpus_bbcOL)\ncorpus_bbcOL_summary$video <- \"BBC\"\n\n\ndf_BI <- read_csv(\"_data/comments_BI.csv\")\n\n\ndf_BI<- df_BI%>% \n  rename(text = \"i\")\n\ndf_BI<- df_BI %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_BI <- df_BI %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_BI<- corpus(df_BI)\ncorpus_BI_summary <- summary(corpus_BI)\ncorpus_BI_summary$video <- \"Business Insider\"\n\ndf_fra <- read_csv(\"_data/comments_fra.csv\")\n\n\ndf_fra<- df_fra%>% \n  rename(text = \"i\")\n\ndf_fra<- df_fra %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_fra <- df_fra %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_fra<- corpus(df_fra)\ncorpus_fra_summary <- summary(corpus_fra)\ncorpus_fra_summary$video <- \"France 21\"\n\ndf_H <- read_csv(\"_data/comments_H.csv\")\n\n\ndf_H<- df_H%>% \n  rename(text = \"i\")\n\ndf_H<- df_H %>%\n  select(text) %>% \n  mutate(text = str_remove_all(text,\"39\"))\n\ndf_H <- df_H %>% \n  select(text) %>% \n  mutate(text = str_remove_all(text,\"<\")) %>% \n  mutate(text = str_remove_all(text,\">\"))\n\ncorpus_H<- corpus(df_H)\ncorpus_H_summary <- summary(corpus_H)\ncorpus_H_summary$video <- \"Harris\"\n\n\n\n\nCode\nfull_df <- rbind(df_bbc,df_q,df_qRev,df_sky,df_bbcQ,df_bbcOL,df_BI,df_fra,df_H)\nfull_df$id <- 1:nrow(full_df)\nfull_df$id <- as.character(full_df$id)\n\nhead(full_df)\n\n\n# A tibble: 6 × 2\n  text                                                                     id   \n  <chr>                                                                    <chr>\n1 Looking forward to it if you just act normal respect the culture and co… 1    \n2 Honestly every country in the world has done bad. May Allah bless these… 2    \n3 if you don't like it stay home and the last people talking about human … 3    \n4 So we can’t boycott a football game ( a trivial matter)  to protest a r… 4    \n5 I see everyone kept their mouth shut with Russia&#;s world cup           5    \n6 Did Qatar invade any country and kill millions? Who are robbing Africa?… 6    \n\n\nCode\nfull_corpus <- corpus(full_df$text)\nfull_corpus_summary<- summary(full_corpus)\n\n\n\n\nCode\ntokens1 <- tolower(full_df$text)\n\n# performs tokenization\ntokens1 <- word_tokenizer(tokens1,pos_remove = c(\"PUNCT\", \"DET\", \"ADP\", \"SYM\", \"PART\", \"AUX\" ))\n\nhead(tokens1, 2)\n\n\n[[1]]\n [1] \"looking\"    \"forward\"    \"to\"         \"it\"         \"if\"        \n [6] \"you\"        \"just\"       \"act\"        \"normal\"     \"respect\"   \n[11] \"the\"        \"culture\"    \"and\"        \"country\"    \"and\"       \n[16] \"you\"        \"will\"       \"be\"         \"ok\"         \"no\"        \n[21] \"one\"        \"needs\"      \"to\"         \"know\"       \"someone\"   \n[26] \"is\"         \"gay\"        \"just\"       \"like\"       \"no\"        \n[31] \"one\"        \"needs\"      \"to\"         \"know\"       \"if\"        \n[36] \"you\"        \"hetero\"     \"just\"       \"save\"       \"that\"      \n[41] \"for\"        \"behind\"     \"closed\"     \"doors\"      \"and\"       \n[46] \"make\"       \"sure\"       \"it\"         \"s\"          \"between\"   \n[51] \"consenting\" \"adults\"     \"children\"   \"and\"        \"animals\"   \n[56] \"are\"        \"not\"        \"consenting\" \"adults\"     \"btw\"       \n[61] \"you\"        \"perverts\"  \n\n[[2]]\n [1] \"honestly\"  \"every\"     \"country\"   \"in\"        \"the\"       \"world\"    \n [7] \"has\"       \"done\"      \"bad\"       \"may\"       \"allah\"     \"bless\"    \n[13] \"these\"     \"workers\"   \"who\"       \"have\"      \"done\"      \"so\"       \n[19] \"much\"      \"for\"       \"the\"       \"country\"   \"however\"   \"to\"       \n[25] \"call\"      \"it\"        \"slavery\"   \"is\"        \"too\"       \"far\"      \n[31] \"they\"      \"r\"         \"not\"       \"taking\"    \"them\"      \"without\"  \n[37] \"there\"     \"will\"      \"like\"      \"when\"      \"british\"   \"and\"      \n[43] \"americans\" \"killed\"    \"and\"       \"took\"      \"thousands\" \"of\"       \n[49] \"slaves\"    \"they\"      \"come\"      \"here\"      \"for\"       \"better\"   \n[55] \"options\"   \"of\"        \"course\"    \"it\"        \"can\"       \"be\"       \n[61] \"better\"    \"however\"   \"it\"        \"is\"        \"much\"      \"better\"   \n[67] \"than\"      \"what\"      \"was\"       \"happening\" \"to\"        \"them\"     \n[73] \"in\"        \"their\"     \"own\"       \"countries\"\n\n\n\n\nCode\nfull_tokens <- tokens(full_corpus,\n    remove_numbers = T,\n    remove_url = T,\n    remove_punct = T,\n    remove_symbols = T)\nfull_tokens <-tokens_tolower(full_tokens)\nfull_tokens <- tokens_select(full_tokens, \n                              pattern = c(stopwords(\"en\"),\"quot\",\"href\",\"don\"),\n                              selection = \"remove\",\n                              min_nchar = 3)\n\n\nhead(full_tokens, 2)\n\n\nTokens consisting of 2 documents.\ntext1 :\n [1] \"looking\" \"forward\" \"just\"    \"act\"     \"normal\"  \"respect\" \"culture\"\n [8] \"country\" \"one\"     \"needs\"   \"know\"    \"someone\"\n[ ... and 22 more ]\n\ntext2 :\n [1] \"honestly\" \"every\"    \"country\"  \"world\"    \"done\"     \"bad\"     \n [7] \"may\"      \"allah\"    \"bless\"    \"workers\"  \"done\"     \"much\"    \n[ ... and 25 more ]\n\n\n\n\nWordClouds and TF-IDF\nMy first step is to do some exploratory analysis to see the most frequent and important terms in my corpus. TF-IDF is intended to measure how important a word is to a document in a collection (or corpus) of documents. I am looking for high TF-IDF values. Using a word cloud will display the frequency of my terms in my corpus.\n\n\nCode\nset.seed(1245)\n\n\nfull_dfm <- dfm(full_tokens)\n\n\nsmaller_dfm <- dfm_trim(full_dfm, min_termfreq = 2)\n\n\nfull_dfm_tfidf <- dfm_tfidf(smaller_dfm)\n\n\ntextplot_wordcloud(smaller_dfm, min_count = 40, random_order = FALSE)\n\n\n\n\n\n\n\nCode\ntopfeatures(full_dfm_tfidf,50)\n\n\n     qatar      world     people        cup    workers    country  countries \n 282.35169  278.88022  249.95985  218.45006  213.95836  205.22592  200.27005 \n      just       like        can      human        one       work     rights \n 192.69143  189.64580  188.09018  154.31723  154.28810  151.48861  147.48920 \n      even       fifa   football    respect        see        get     really \n 130.13915  128.56978  127.23205  122.62079  121.78159  119.58720  112.11300 \n      good      video    western       know       many    migrant      money \n 109.62160  108.02355  107.75301  107.31365  107.28179  106.03057  105.38019 \n       now      years       make       time       west       also       want \n 103.36550  102.87957  101.11145   99.35274   99.19814   98.19723   97.12021 \n   culture conditions       much      never       well     middle        say \n  96.77193   94.56548   93.12160   92.59565   91.39109   87.47787   87.47787 \n     think       love      still      every    working      going      thing \n  86.93365   86.49862   86.06694   86.02204   85.58083   84.17787   82.79395 \n     india \n  82.36553 \n\n\nThis word cloud depicts the frequency of words that were counted more than 40 times in the corpus. Words like workers, culture, migrant, western, rights and human shown in the word cloud imply that the common discussion within these comments could be about the clash of cultural difference between visitors of the world cup and the people who live in Qatar. The word workers being large suggests that the discussion of how workers were treated during the build up of the World Cup is common in the corpus. Based on TF_IDF ranking, I pulled the top 50 most important terms from my corpus. Numerically, I can see words like workers, fifa, respect, rights, human and culture are ranked highly in my corpus.\n\n\nSentiment Analysis\nFor Sentiment Analysis I used the packages tidytext and sentimentr. Sentimentr attempts to take into account valence shifters (i.e., negators, amplifiers (intensifiers), de-amplifiers (downtoners), and adversative conjunctions). This will give me better results than before. I then pulled the most positive and negative comments from the corpus to analyze.\n\n\nCode\nlibrary(sentimentr)\n\nmytext <- get_sentences(full_df$text)\nsenti<- sentiment_by(mytext)\n\nsenti<- senti %>% \n  filter(word_count > 4)\n\nqplot(senti$ave_sentiment,   geom=\"histogram\",binwidth=0.1,main=\"Sentiment Histogram\")\n\n\n\n\n\n\n\nCode\nwriteLines(head(full_corpus[which(senti$ave_sentiment >.5)]))\n\n\na country that has modern day slavery, human rights abuses, sharia law which is extremely discriminatory to women and people of any other religion other than Islam and Christianity, one of the highest death rates of workers, yet it&#;s okay to play football there, amazing isn&#;t it, people say money can&#;t buy you everything, show them this shitshow\nThis Arabic nation is little humanitarian concepts among workers , especially  home maid workers conditions are very brutal no human rights and laws. like fisherman attitude.\nA golden opportunity to combat negative stereotypes... used to make them worse.\nChad Qatar : Bribes Virgin West to submission. Humiliaties the West. Doesn&#;t care about stupid lgbt flag. Profits\nnow do the same when US host a worldwide event lol\nIm glad someone made a vid about this, I lived in Doha for 8 years and recently left back to my country. Whenever I would go home from school you could see workers being forced to work in the summers peak temp hour even though it was made illegal to work from 12pm to 4pm or something like that? Theres also a huge lack of safety, cranes with cargo would be moved OVER MOVING TRAFFIC:brbrTo any adults who plan to go to the world cup I will tell you now that many places in Qatar will not serve alchoholic drinks because they need a license and can only purchase their alchohol from QDC. So if you do want alchohol I&#;d reccommend going to a hotel like the Grand Hyatt or something. So don&#;t get upset if ya cant find a place to get beer. (but in the stadium there  will be alchohol drinking zones).\n\n\n\n\nCode\nwriteLines(head(full_corpus[which(senti$ave_sentiment < -0.5)]))\n\n\nIf this was Russia most teams would have pulled out!\nWhen is the west going to respecting other countries without pushing their agenda, I mean we all know this ain’t about the workers sharing rooms or the stadium being moveable ( it’s about the west pushing so called LGBQT) no wonder why only the west condemned Russia and the world didn’t\nFertilizer I am a Qatari expatriate. There are many workers in Qatar who work for 6/7 years. They work for the old wages. The company does not increase their wages. Many people have been able to change their jobs and have benefited a lot from the introduction of government without noc system. If the Qatari government had given the opportunity to the workers who have been working in Qatar for more than 5 years to change the kafala without noc, the workers in Qatar would have benefited a lot.\nHalf the British clowns are mad because, they don’t get to drink 🤡\nIs the homophobic, slave-using, dictatorship of Qatar`s lack of feeling shame news ?\nQatar should improve migrants workers conditions.  SHAME on you Qatar\n\n\nThe overall sentiment of the corpus is skewed right, suggesting that most of the comments are negative. The most positive comments show more western culture beliefs and criticisms of Qatar. The reasoning and language the commenters use are socially acceptable based on their knowledge of the situation in Qatar and their experiences. Some of the comments are jokes but the main points get across about their beliefs. The most negative comments are more hateful in their beliefs about the opposing culture. The commenters are not using any reasonable judgment to make their claims.\n\n\nDictionary Analysis\nI chose to use the NRC dictionary to visualize the contribution of terms to emotional sentiment.\n\n\nCode\npost_clean <- full_df %>%\n  select(text) %>%\n  unnest_tokens(word, text) %>% \n  filter(!word %in% stop_words$word) %>% \n  filter(word != \"https\") %>% \n  filter(word != \"href\") %>% \n  filter(word != \"www.youtube.com\")\n\nsentiment_word_counts <- post_clean %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., get_sentiments(\"nrc\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 4 of `x` matches multiple rows in `y`.\nℹ Row 1506 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nCode\nsentiment_word_counts<- sentiment_word_counts %>% \n  filter(word != \"don\")\n\n\nsentiment_word_counts %>%\n  group_by(sentiment) %>%\n  top_n(9) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(title = \"Sentiment terms\",\n       y = \"Contribution to sentiment\",\n       x = NULL) +\n  coord_flip()\n\n\nSelecting by n\n\n\n\n\n\nI can see that the words “slavery”, “government” , “Corruption”, and “treat” dominate negative emotions. Anticipation and Anger are two emotions I want to look at because they relate well to the lead up of the World Cup. Seeing “respect” contribute to the emotion Anticipation could associate that fans are expecting respect of all cultures and people at the World Cup. The emotion Anger reveals the words money and politics are contributing to the emotional distress of fans.\n\n\nTopic Model\nFor LDA topic Modeling, I used the package text2vec. I found that the best value for K ranges form 5-10 from the my last blog post. I then extract the top 10 words from my topics. In this example, I set k=7.\n\n\nCode\nit1 <- itoken(tokens1, ids = full_df$id, progressbar = FALSE)\n\nstop_words1 = c(\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\"a\",\"the\",\"in\",\"as\",\"on\", \"is\",\"it\", \"to\",\"of\",\"are\",\"not\",\"and\",\"quot\",\"don\",\"youtu.be\",\"an\",\"have\",\"this\",\"if\",\"they\",\"v\",\"2\",\"their\",\"can\", \"than\",\"ve\")\nv1 <- create_vocabulary(it1,stopwords = stop_words1)\n\nv1\n\n\nNumber of docs: 1391 \n37 stopwords: i, me, my, myself, we, our ... \nngram_min = 1; ngram_max = 1 \nVocabulary: \n        term term_count doc_count\n   1:   0.25          1         1\n   2:  0m58s          1         1\n   3:    1.2          1         1\n   4:    1.5          1         1\n   5: 10,000          1         1\n  ---                            \n5962: people        333       247\n5963:  world        425       307\n5964:  qatar        481       360\n5965:   that        609       362\n5966:    for        636       417\n\n\nCode\n#v1 <- prune_vocabulary(v1, term_count_min = 5)\n\n\n\nvectorizer1 <- vocab_vectorizer(v1)\n\ndtm1 <- create_dtm(it1, vectorizer1, type = \"dgTMatrix\")\n\nlda_model1 <- LDA$new(n_topics = 5, doc_topic_prior = 0.1,\n                     topic_word_prior = 0.01)\n\n\ndoc_topic_distr1 <- \n  lda_model1$fit_transform(x = dtm1, n_iter = 1000,\n                          convergence_tol = 0.001, n_check_convergence = 25,\n                          progressbar = FALSE)\n\n\nINFO  [17:24:47.260] early stopping at 150 iteration\nINFO  [17:24:47.811] early stopping at 75 iteration\n\n\nCode\nlda_model1$get_top_words(n = 10,\n                        lambda = 0.1)\n\n\n      [,1]         [,2]      [,3]              [,4]      [,5]        \n [1,] \"east\"       \"2022\"    \"watch\"           \"got\"     \"must\"      \n [2,] \"india\"      \"worker\"  \"https\"           \"problem\" \"million\"   \n [3,] \"nations\"    \"shame\"   \"href\"            \"fact\"    \"qatari\"    \n [4,] \"anything\"   \"issue\"   \"change\"          \"videos\"  \"through\"   \n [5,] \"day\"        \"cup\"     \"war\"             \"labor\"   \"truth\"     \n [6,] \"doing\"      \"another\" \"gay\"             \"ago\"     \"end\"       \n [7,] \"away\"       \"build\"   \"www.youtube.com\" \"n\"       \"violations\"\n [8,] \"called\"     \"kind\"    \"boycott\"         \"yeah\"    \"worked\"    \n [9,] \"indian\"     \"next\"    \"game\"            \"less\"    \"off\"       \n[10,] \"population\" \"africa\"  \"her\"             \"long\"    \"business\"  \n\n\n\n\nCode\nlda_model1$plot()\n\n\nLoading required namespace: servr\n\n\nThe Topic Model implies that the major topic of discussion surrounds how migrant workers were treated. The other topics in the model suggests respecting the laws of the host country, violation of human rights and government values.The first topic specifically is about where workers came from and how they were treated. Topic 6 specifically highlights the discussion of respect of a countries’ laws and culture.\n\n\nSemantic and Pairwise Correlation Network\nThe packages I used for Semantic and Pairwise Analysis include quanteda, widyr and ggraph. Here I wanted to learn a bit more about what features co-occur and correlation among words, by creating networks for both will help me examine this.\n\n\nCode\nfull_fcm <- fcm(smaller_dfm)\n\n# keep only top features.\nsmall_fcm <- fcm_select(full_fcm, pattern = names(topfeatures(full_fcm, 60)), selection = \"keep\")\n\n# compute weights.\nsize <- log(colSums(small_fcm))\n\n# create network.\ntextplot_network(small_fcm, vertex_size = size / max(size) * 4)\n\n\n\n\n\nHere I created a Semantic Network of the top 60 terms in the FCM. I see the heart of the network revolves around political terms. The two terms political and war seem to co occur with a lot of other terms. This could imply that some comments are discussion a war between different political views .I also see a sub network that links travelers to law,laws and alcohol. This implies that people visiting the world cup must follow the laws in place, especially the no alcohol law.\nHere I want to examine correlation among words, which indicates how often they appear together relative to how often they appear separately. The pairwise_cor() function in widyr lets us find the phi coefficient between words based on how often they appear in the same section. Here I pick particular terms of interest and find the other terms most associated with them and create a visualization of the correlations and clusters of words.\n\n\nCode\nsection_words <- full_df %>%\n  mutate(section = row_number() %/% 10) %>%\n  filter(section > 0) %>%\n  unnest_tokens(word, text) %>%\n  filter(!word %in% stop_words$word) %>% \n  filter(word != \"https\") %>% \n  filter(word != \"href\") %>% \n  filter(word != \"www.youtube.com\") %>%\n  filter(word != \"youtu.be\") %>% \n  filter(word!= \"3\") %>% \n  filter(word!= \"2\") %>% \n  filter(word!= \"1\") %>% \n  filter(word!= \"12\") %>% \n  filter(word!= \"ve\")\n  \n  \n\nlibrary(widyr)\nlibrary(ggraph)\n\n\nword_cors <- section_words %>%\n  group_by(word) %>%\n  filter(n() >= 15) %>%\n  pairwise_cor(word, section, sort = TRUE)\n\n\n\nset.seed(2016)\n\nword_cors %>%\n  filter(correlation > .35) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +\n  geom_node_point(color = \"lightblue\", size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  theme_void()\n\n\n\n\n\nCode\nword_cors %>%\n  filter(item1 %in% c(\"western\", \"qatar\", \"lgbtq\", \"workers\",\"rights\",\"respect\",\"country\",\"cultures\")) %>%\n  group_by(item1) %>%\n  slice_max(correlation, n = 6) %>%\n  ungroup() %>%\n  mutate(item2 = reorder(item2, correlation)) %>%\n  ggplot(aes(item2, correlation)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ item1, scales = \"free\") +\n  coord_flip()\n\n\n\n\n\nThe Words of interest that I picked include “western”, “qatar”, “lgbtq”, “workers”,“rights”,“respect”,“country”,“cultures”. The correlation between the words respect and culture, politics suggests that respecting the culture and politics of a certain region is being discussed when talking about respect. The Visualization shows the correlation between many words. The relationships here are symmetrical, rather than directional. The connections between words help verify what topics are being discussed in all of the comments. For example, the cluster surrounding the word “worker” shows that the word is correlated with negative terms that relate to the treatment and condition they received.\n\n\nLimitations\nThe conclusions drawn cannot be fully proven from the current data , but they are enough to encourage further research.The data was only collected in a small scope compared to other projects. This is because of Youtube API did not let me scrape more than 100 comments per video. Many comments had typos in them, which hurts my analysis because some of the misspelled words could of been valuable. The topics of the videos may have skewed the conversation of the comments because the videos come from different sources."
  },
  {
    "objectID": "Nayan.html",
    "href": "Nayan.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "Education\nUniversity of Massachusetts Amherst, College of Social and Behavioral Sciences, Amherst, MA. Exp. Graduation:: Feb 2024 Master of Science Concentration: Data Analytics and Computational Social Science Current GPA: 3.89 Relevant Courses: Advanced Quantitative Methods, Regression Models, Text as Data, Advanced Data-Driven Storytelling\nUniversity of Rhode Island, College of Arts and Sciences, Kingston, RI. Graduated:: May 2022 Bachelor of Science Concentration: Data Science Final GPA: 3.52, Dean’s List: 2018, 2019, 2020, 2021, 2022 Relevant Courses: Machine Learning, Multivariate Statistical Learning, Big Data Analysis, Database Management\n\n\nSkills\nGeneral Skills: Data Analysis, Machine Learning, Data Visualization, Data Storytelling, Data Cleaning, NLP\nProgramming Languages: Python, R, and SQL\nLibraries: Ggplot2, Tidyverse, Summarytools, Stats, Tidytext, Quanteda, Rselenium, Lubridate, Devtools, SciPy, Matplotlib, Seaborn, Scikit-learn, Numpy, Pandas\n\n\nProjects\nU.S. Job Satisfaction: Impacts of Technology, Work Environment, and Covid-19 September-December 2023\n\nInvestigated if the move to more work from home and more use of technology at work post-COVID actually lead to any significant changes in job satisfaction using data from the 2018 and 2022 versions of the General Social Survey.\nDeveloped two ordered logit GLMs using the MASS package in R: one main effect model and one interaction model\nCalculated predicted probabilities of job satisfaction for all significant variables (p <0.05) using ggpredict() and graphed them with 95% confidence intervals using ggplot().\nDiscovered that people who never work from home are significantly less likely to to be very satisfied with their job than people who mainly WFH (p <0.05), with all variables held constant.\n\nLink to paper: U.S. Job Satisfaction: Impacts of Technology, Work Environment, and Covid-19\nIdentifying Sources of Poor Nutrition for Americans, Amherst, MA July-August 2023 Project Owner - Analyzed the impact of food sources on food and nutrient intakes for different ages and income levels using 2017-18 Food and Nutrient Density by Food Source and Demographic Characteristics datasets - Created multiple charts that visualized the 3 way associations between food source, demographics and food/nutrient density using\nggplot2 in R - Discovered that adults and seniors were consuming more cholesterol at restaurants than at home, low and middle income individuals had lower intakes of protein at home than away from home - Cleaned and combined datasets using the tidyverse package in R in order to produce visualizations - Explained methods and results using language that a non-technical audience could understand in a 29 page report\nLink: Identifying Sources of Poor Nutrition for Americans\nFixing Social Media Design Brief February-May 2023 Team Member - Developed a design brief for an app called “Social Media Royale” that helps reduce people’s screen time with 2 student colleagues - Handled background research and interviews over Zoom to understand the consequences of high amounts of screen time - Brainstormed many design ideas that were is used in the final pitch with my team members - Gained substantial knowledge about the process of product design\nLink: Fixing Social Media Design Brief\nResearch Design Final Project February-May 2023 Team Member - Conducted a survey experiment research study with the goal of understanding if exposure to misinformation increases belief in conspiracy theories with two student colleagues - Defined constructs, operational definitions, ways of measurement for constructs, and treatment groups for the research study - Used Qualtrics for distributing the survey - Analyzed results from the survey using stacked bar charts, One-Way Anova, and Linear Regression using R\nLink: Misinformation and Conspiracy Theories\nNBA Salary Prediction, Amherst, MA February-May 2023 Project Owner - Developed and tested 4 machine learning models using different regression methods to explore which one performs the best at predicting NBA players’ salaries using R and Python - Found that Random Forest performed the best out of all methods, yielding a low RMSLE of 0.50 - Pre-processed and wrangled data into a suitable format for the ML models - Tuned hyperparameters for each model using GridSearchCV with 5 folds to control for overfitting - Presented my work at my program’s research symposium in front of faculty and peers\nLink: NBA Salary Prediction\nGoals in Soccer Regression Models Project, Amherst, MA November-December 2022\nTeam Member - Investigated the difference in total goals across Europe’s top 5 soccer leagues using team data from the 2021-22 season to understand if the type of league determines the number of goals per season - Developed a Quasi-Poisson regression model with 9 predictors using R with 3 student colleagues - Concluded that there was a significant difference in goals between Bundesliga and other leagues - Handled the the selection of predictors by creating and analyzing a scatter-plot matrix using GGally - Coordinated weekly group discussions with team members on Zoom and in person\nLink: Goals in Soccer\nAssessing Sentiment Surrounding the 2022 World Cup, Amherst, MA September-December 2022 Project Owner - Analyzed controversies of the Qatar World Cup by conducting Sentiment Analysis, LDA Topic Modeling, Semantic Network, and Pairwise\nCorrelation Analysis on Youtube comments using R - Concluded that the overall sentiment of the comments was negative, main focus of discussion surrounded human rights violations - Used packages tidyverse, quanteda, tidytext, text2vec, LDAvis, and sentimentr to conduct analysis - Extracted 1,391 comments in total from 9 Youtube videos using Youtube API in Python\nMy final conclusions were made on my final poster.\nFinal Poster: Assessing Sentiment Surrounding the 2022 World Cup\nSpring 2022 Machine Learning Project at URI, Kingston, RI Research Assistant - Designed a program in Python that estimated heterogeneous treatment effects using datasets that involve AIDS and breast cancer treatments as a part of my senior dissertation - Implemented different meta learners in my program that could estimate the conditional average treatment effect to identify how to personalize treatment regimes - Gained experience and knowledge in Causal Inference - Discussed methodology weekly with research leader\n\n\nExperience\nSummer 2021 Bora, La Jolla, CA Business Development & Land Acquisition - Compiled data about potential locations using Google Maps - Analyzed aggregated data to determine which locations are the busiest based on area busyness - Company’s goal is to bring to market an app based, self service beach chair rental service stationed at the most popular locations across the country - Part of outreach campaign to early stage VC and angel investors\nSpring 2018 Westford Academy Spring Recreational Basketball, Westford, MA Youth Basketball Coach/League Co-Director - Organized and operated a Spring League consisting of 80 players with 3 student colleagues as part of my senior year internship - Successfully coordinated a three month league for the Westford Youth Association. All surplus proceeds were donated back to the Westford Academy Girls Basketball Program - Established online sign-up and jersey purchases - Coordinated facilities and scheduling of refereeing staff\n\n\nHometown\nWestford, MA"
  },
  {
    "objectID": "NBA_Salary.html",
    "href": "NBA_Salary.html",
    "title": "NBA Salary Prediction",
    "section": "",
    "text": "Code\nlibrary(alr4)\nlibrary(tinytex)\nlibrary(summarytools)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(aod)\nlibrary(DescTools)\nlibrary(MASS)\nlibrary(leaps)\nlibrary(GGally)\nlibrary(hrbrthemes)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "NBA_Salary.html#introduction",
    "href": "NBA_Salary.html#introduction",
    "title": "NBA Salary Prediction",
    "section": "Introduction",
    "text": "Introduction\nBeing a General Manager in the NBA comes with a ton of decisions. One of the most important decisions a GM can make is how much they pay the players on their team. It is so important to pay the players the right amount in order to build the strongest roster. Overpaying a player will hurt a teams cap space, meaning that the team will not be able to sign good players because they do not have enough money to afford them. My motivation for this project its to see if Machine Learning techniques can correctly predict a players salary. The idea is if I am able to create a model that performs well enough, then it could be used as a tool to determine a players salary for their next contract. Here I will perform different regression methods to predict players salary and then used the best method for prediction."
  },
  {
    "objectID": "NBA_Salary.html#the-data",
    "href": "NBA_Salary.html#the-data",
    "title": "NBA Salary Prediction",
    "section": "The Data",
    "text": "The Data\nThe dataset I am using comes from Kaggle. The dataset contains information about player names, time span of the contract, avg salary per year and all stats that player accumulated during NBA season before signing their next contract. The scope of the data is as follows: - There are only contracts signed since 2010/2011 season to 2019/2020 season. - Only includes players that are active in 2020/2021 season. - Doesn’t include rookie or retained contracts. - Doesn’t include contracts for player that haven’t played year before the signing the contract.\nThis is a good scope because I want to use modern players contracts for future predictions. The limitation of only including players that are active in 20/21 means that these players were able to earn multiple contracts of the 10 year span, which validates them as players who are worth to continuing paying. Having this removes players who had massive contracts early in their career and then faded out quickly after their primes.\n\n\nCode\ndf<- read_csv(\"_data/nba_contracts_history.csv\")\n\n\nRows: 199 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (27): CONTRACT_START, CONTRACT_END, AVG_SALARY, AGE, GP, W, L, MIN, PTS,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndf\n\n\n# A tibble: 199 × 28\n   NAME  CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n   <chr>   <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Wesl…    2019    2020  2.56e6    32    69    27    42  2091   840   279   698\n 2 Broo…    2015    2017  2.12e7    27    72    34    38  2100  1236   506   987\n 3 DeAn…    2011    2014  1.08e7    22    80    31    49  2047   566   234   341\n 4 Mark…    2015    2018  8.14e6    25    82    39    43  2581  1258   512  1100\n 5 Dwig…    2018    2019  1.34e7    32    81    35    46  2463  1347   506   911\n 6 Aust…    2015    2016  7.06e6    22    76    46    30  1563   530   203   496\n 7 Wayn…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n 8 JaMy…    2019    2020  4.77e6    29    65    31    34  1371   611   230   476\n 9 Kyle…    2015    2018  4.05e6    25    51    15    36   824   294   119   242\n10 Trev…    2014    2017  8   e6    28    77    41    36  2723  1107   389   853\n# … with 189 more rows, 16 more variables: `FG%` <dbl>, `3PM` <dbl>,\n#   `3PA` <dbl>, `3P%` <dbl>, FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>,\n#   DREB <dbl>, REB <dbl>, AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>,\n#   PF <dbl>, `+/-` <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\n\n\nCode\ndf<- df %>% mutate(c_duration = CONTRACT_END -CONTRACT_START)\n\n\nAfter loading in the data I can see it has 199 instances with 29 features. Some features will be removed such as player name, CONTRACT_START and CONTRACT_end. After EDA I will remove any features that interfere with my regression analysis. The only variable mutation I did was c_duration, which is how long a players contract lasted."
  },
  {
    "objectID": "NBA_Salary.html#eda",
    "href": "NBA_Salary.html#eda",
    "title": "NBA Salary Prediction",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nggplot(data=df, aes(x=AGE, y=AVG_SALARY)) +\n  geom_bar(stat=\"identity\") +\n  labs(title = \"Average Salary by Age\", x = \"Age\" ,y= \"Average Salary\")\n\n\n\n\n\nThe first relationship I wanted to visualize was Average salary and Age. Age is very important when thinking about how much to pay someone because you want to give them a contract that shows how good they can perform for that length of contract. I can see that average salary peaks at age 23 and again rises at 27 and then 30. This make sense because young players who are really good will receive a massive 2nd contract (contract after rookie deal). The peaks at 27 and 30 could be from players signing their 3rd or 4th contracts but make less money due to their age.\n\n\nCode\nprint(dfSummary(df, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ndf\nDimensions: 199 x 29\n  Duplicates: 1\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      NAME\n[character]\n      1. Kevin Durant2. Austin Rivers3. Avery Bradley4. Danilo Gallinari5. Dwight Howard6. E'Twaun Moore7. Paul Millsap8. Quinn Cook9. Robin Lopez10. Wayne Ellington[ 128 others ]\n      4(2.0%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)168(84.4%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_START\n[numeric]\n      Mean (sd) : 2015.2 (2.1)min ≤ med ≤ max:2011 ≤ 2015 ≤ 2019IQR (CV) : 3 (0)\n      2011:9(4.5%)2012:16(8.0%)2013:16(8.0%)2014:21(10.6%)2015:48(24.1%)2016:35(17.6%)2017:23(11.6%)2018:20(10.1%)2019:11(5.5%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_END\n[numeric]\n      Mean (sd) : 2017.5 (1.7)min ≤ med ≤ max:2013 ≤ 2018 ≤ 2020IQR (CV) : 3 (0)\n      2013:2(1.0%)2014:11(5.5%)2015:18(9.0%)2016:23(11.6%)2017:29(14.6%)2018:46(23.1%)2019:55(27.6%)2020:15(7.5%)\n      \n      0\n(0.0%)\n    \n    \n      AVG_SALARY\n[numeric]\n      Mean (sd) : 11073609 (7897820)min ≤ med ≤ max:823244 ≤ 9500000 ≤ 33599500IQR (CV) : 11621898 (0.7)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AGE\n[numeric]\n      Mean (sd) : 25.9 (2.8)min ≤ med ≤ max:20 ≤ 25 ≤ 36IQR (CV) : 4 (0.1)\n      16 distinct values\n      \n      0\n(0.0%)\n    \n    \n      GP\n[numeric]\n      Mean (sd) : 64.2 (19.6)min ≤ med ≤ max:1 ≤ 72 ≤ 82IQR (CV) : 19 (0.3)\n      59 distinct values\n      \n      0\n(0.0%)\n    \n    \n      W\n[numeric]\n      Mean (sd) : 34.2 (14.5)min ≤ med ≤ max:0 ≤ 35 ≤ 64IQR (CV) : 20 (0.4)\n      56 distinct values\n      \n      0\n(0.0%)\n    \n    \n      L\n[numeric]\n      Mean (sd) : 30 (13)min ≤ med ≤ max:0 ≤ 31 ≤ 62IQR (CV) : 18.5 (0.4)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      MIN\n[numeric]\n      Mean (sd) : 1747 (782.4)min ≤ med ≤ max:2 ≤ 1867 ≤ 3125IQR (CV) : 1125.5 (0.4)\n      193 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PTS\n[numeric]\n      Mean (sd) : 813.4 (499.9)min ≤ med ≤ max:0 ≤ 734 ≤ 2376IQR (CV) : 710.5 (0.6)\n      186 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGM\n[numeric]\n      Mean (sd) : 300.4 (178.6)min ≤ med ≤ max:0 ≤ 277 ≤ 743IQR (CV) : 261 (0.6)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGA\n[numeric]\n      Mean (sd) : 638.8 (374.6)min ≤ med ≤ max:1 ≤ 572 ≤ 1643IQR (CV) : 553.5 (0.6)\n      189 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FG%\n[numeric]\n      Mean (sd) : 46.7 (8.1)min ≤ med ≤ max:0 ≤ 45.5 ≤ 100IQR (CV) : 7 (0.2)\n      131 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PM\n[numeric]\n      Mean (sd) : 63.6 (58.4)min ≤ med ≤ max:0 ≤ 57 ≤ 272IQR (CV) : 91 (0.9)\n      111 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PA\n[numeric]\n      Mean (sd) : 173.9 (148.7)min ≤ med ≤ max:0 ≤ 170 ≤ 657IQR (CV) : 241 (0.9)\n      139 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3P%\n[numeric]\n      Mean (sd) : 29.7 (13.2)min ≤ med ≤ max:0 ≤ 34.8 ≤ 50IQR (CV) : 11.9 (0.4)\n      110 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTM\n[numeric]\n      Mean (sd) : 149 (128.4)min ≤ med ≤ max:0 ≤ 107 ≤ 720IQR (CV) : 143 (0.9)\n      143 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTA\n[numeric]\n      Mean (sd) : 195.5 (162.2)min ≤ med ≤ max:0 ≤ 145 ≤ 837IQR (CV) : 192.5 (0.8)\n      162 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FT%\n[numeric]\n      Mean (sd) : 74 (15)min ≤ med ≤ max:0 ≤ 76.8 ≤ 100IQR (CV) : 11.4 (0.2)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OREB\n[numeric]\n      Mean (sd) : 79.3 (72.7)min ≤ med ≤ max:0 ≤ 51 ≤ 397IQR (CV) : 85.5 (0.9)\n      117 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DREB\n[numeric]\n      Mean (sd) : 249 (164)min ≤ med ≤ max:1 ≤ 209 ≤ 829IQR (CV) : 235 (0.7)\n      167 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REB\n[numeric]\n      Mean (sd) : 328.3 (226.1)min ≤ med ≤ max:1 ≤ 286 ≤ 1226IQR (CV) : 305 (0.7)\n      169 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AST\n[numeric]\n      Mean (sd) : 171.7 (163.9)min ≤ med ≤ max:0 ≤ 110 ≤ 839IQR (CV) : 137 (1)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      TOV\n[numeric]\n      Mean (sd) : 103.4 (70.9)min ≤ med ≤ max:0 ≤ 85 ≤ 374IQR (CV) : 83 (0.7)\n      138 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STL\n[numeric]\n      Mean (sd) : 58.2 (37.3)min ≤ med ≤ max:0 ≤ 48 ≤ 169IQR (CV) : 43 (0.6)\n      100 distinct values\n      \n      0\n(0.0%)\n    \n    \n      BLK\n[numeric]\n      Mean (sd) : 39.6 (43.3)min ≤ med ≤ max:0 ≤ 23 ≤ 269IQR (CV) : 40 (1.1)\n      83 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PF\n[numeric]\n      Mean (sd) : 137.7 (64.3)min ≤ med ≤ max:1 ≤ 137 ≤ 291IQR (CV) : 84 (0.5)\n      135 distinct values\n      \n      0\n(0.0%)\n    \n    \n      +/-\n[numeric]\n      Mean (sd) : 62.9 (227.7)min ≤ med ≤ max:-628 ≤ 44 ≤ 839IQR (CV) : 257.5 (3.6)\n      171 distinct values\n      \n      0\n(0.0%)\n    \n    \n      c_duration\n[numeric]\n      Mean (sd) : 2.3 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.5)\n      1:69(34.7%)2:36(18.1%)3:66(33.2%)4:28(14.1%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.0)2024-01-08\n\n\n\nHere I created summary statistics of the dataset using SummaryTools. The two stats that stood out to me was the mean games played and mean minutes played. The GP and MIN mean values are 64.2 and 1747, respectively. This implies that in order to be considered for another contract, players must play most of the season and play about 21 minutes per game. I would say that if you looked at all the means and medians of each feature, then those values represent a player who will get another NBA contract.\n\n\nCode\nggplot(data = df, mapping = aes(x = MIN, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Minutes Played\", x = \"Mins\" ,y= \"Average Salary\")\n\n\n\n\n\nNext I wanted to check the relationship between Minutes played and Average Salary. I can see that their relationship is non-linear but positive. I see an increase of salary once a player is playing roughly 1750 minutes but drops off around 2500 minutes.The positive relationship could suggest that minutes played can be a good predictor.\n\n\nCode\nggplot(data = df, mapping = aes(x = `+/-`, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Plus/Minus\", x = \"+/-\" ,y= \"Average Salary\")\n\n\n\n\n\nHere I wanted to see the relationship between +/- and Average salary. +/- is a sports statistic used to measure a player’s impact on the game, represented by the difference between their team’s total scoring versus their opponent’s when the player is in the game. I can see the relationship between +/- and Average Salary is non linear. I can see that there are players with awful +/- that are getting payed more than players with high +/-. I also see a lot of data grouped around 0 with low salaries, meaning these players probably did not play much.\n\n\nCode\ndf %>% filter(`+/-` < -300)\n\n\n# A tibble: 7 × 29\n  NAME   CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n  <chr>    <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Wayne…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n2 JaVal…    2012    2015  1.13e7    24    61    22    39  1535   691   307   552\n3 Gordo…    2014    2017  1.90e7    24    77    23    54  2800  1248   426  1032\n4 Jorda…    2016    2019  1.25e7    24    79    17    62  2552  1225   475  1098\n5 Derri…    2014    2017  1.2 e7    22    73    25    48  2201   970   390   747\n6 JaKar…    2015    2016  1.10e6    22    74    17    57  1131   386   146   346\n7 Nikol…    2015    2018  1.2 e7    24    74    21    53  2529  1428   631  1206\n# … with 17 more variables: `FG%` <dbl>, `3PM` <dbl>, `3PA` <dbl>, `3P%` <dbl>,\n#   FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>, DREB <dbl>, REB <dbl>,\n#   AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>, PF <dbl>, `+/-` <dbl>,\n#   c_duration <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\nHere I investigate the players with terrible +/-. Some of these players are getting paid well but their teams are so bad that their +/- statistic is negative. This makes me believe that non-linear regression methods would be useful if this feature is used for prediction of salary.\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 2:7),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 8:13),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 22:26),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 14:20),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\nlibrary(ggcorrplot)\n\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  cor() %>% \n  ggcorrplot(hc.order = TRUE, type = \"lower\",outline.col = \"white\",lab=TRUE, lab_size=1)\n\n\n\n\n\nHere I created 3 ggpairs plots and a correlation matrix to investigate the correlations between my target variable and and features. I am looking for strong and weak correlations. Based on the correlation matrix, I see a mixture or strong and weak correlations between features and my target variable, Average Salary. This makes me believe that non linear regression methods will perform better than linear regression methods on this dataset.\n\n\nCode\ndf %>%  ggplot(aes(x=AVG_SALARY)) + \n  geom_density() +\n  geom_vline(aes(xintercept=mean(AVG_SALARY)),\n            color=\"blue\", linetype=\"dashed\", size=1)\n\n\n\n\n\nCode\ndf %>% dplyr::select(AVG_SALARY) %>% \n  summarise(mean = mean(AVG_SALARY))\n\n\n# A tibble: 1 × 1\n       mean\n      <dbl>\n1 11073609.\n\n\nHere I wanted to look at the distribution of my target variable. I can see the distribution is not normal and skewed right. Seeing this makes me think that more flexible methods will work better when I create my regression models."
  },
  {
    "objectID": "NBA_Salary.html#pre-processing-in-python",
    "href": "NBA_Salary.html#pre-processing-in-python",
    "title": "NBA Salary Prediction",
    "section": "Pre Processing in Python",
    "text": "Pre Processing in Python\n\n\nCode\ndf = pd.read_csv(\"_data/nba_contracts_history.csv\")\n\n\n\n\nCode\n#Creating variable c_duration\ndf[\"c_duration\"] = df[\"CONTRACT_END\"] - df[\"CONTRACT_START\"] \n\n\n\n\nCode\n#Creates set of features I will use\nfeatures = df.drop([\"NAME\",\"CONTRACT_START\", \"CONTRACT_END\",\"AVG_SALARY\",\"W\",\"L\"], axis=1)\n\n\n\n\nCode\n#sub-setting my target variable, Average Salary\ntarget = df['AVG_SALARY'].to_numpy()\n\n\nHere is my data splits, 70% training, 15% validation and 15% Test.\n\n\nCode\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.3, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5, random_state= 42)\n\n\nHere I check to see if the data is split up correctly.\n\n\nCode\nprint(X_train.shape, X_valid.shape, X_test.shape)\n\n\n(139, 23) (30, 23) (30, 23)\n\n\nCode\nprint(Y_train.shape, Y_valid.shape, Y_test.shape)\n\n\n(139,) (30,) (30,)\n\n\nHere I use the make_scorer() function so that I can call RMSLE during Grid Search Cross Validation.\n\n\nCode\nRMSLE = make_scorer(mean_squared_log_error, squared=False)"
  },
  {
    "objectID": "NBA_Salary.html#random-forest",
    "href": "NBA_Salary.html#random-forest",
    "title": "NBA Salary Prediction",
    "section": "Random Forest",
    "text": "Random Forest\nFor hyperparameter tuning, I chose to tune n_estimators, max_features, min_samples_split and min_sample_leafs. From Sckit learn’s documentation, it states that n_estimators and max_features are the main parameters to tune, so I will follow the documentations suggestion. I included min_sample_split and min_sample_leafs because I want to control my model for overfitting. By increasing the values of these two parameters, it will help prevent my model from overfitting.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\n\nparam_grid_RF = {\n    'n_estimators': [100, 200, 300, 400],\n     'max_features': [None, 1.0],\n     'min_samples_split': [2, 4, 6 ,8],\n    'min_samples_leaf': [1, 2 ,4, 6, 8]}\n    \n    \n    \ngridRF= GridSearchCV(RF, param_grid_RF,scoring = RMSLE, cv=5)\ngridRF.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(),\n             param_grid={'max_features': [None, 1.0],\n                         'min_samples_leaf': [1, 2, 4, 6, 8],\n                         'min_samples_split': [2, 4, 6, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridRF.best_params_)\n\n\nBest parameters: {'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRF.best_score_)\n\n\nTraining error (RMSLE): 0.568854831389231\n\n\nChecking Validation Error for Random Forests.\n\n\nCode\n\nbest_model_RF = gridRF.best_estimator_\npredict_y_RF = best_model_RF.predict(X_valid)\n\nRF_rmsle = mean_squared_log_error(Y_valid, predict_y_RF,squared=False)\n\n\nprint(\"Validation error (RMSLE):\",RF_rmsle)\n\n\nValidation error (RMSLE): 0.46956491142816686"
  },
  {
    "objectID": "NBA_Salary.html#scaling-data",
    "href": "NBA_Salary.html#scaling-data",
    "title": "NBA Salary Prediction",
    "section": "Scaling Data",
    "text": "Scaling Data\nScaling data for SVM and Ridge Regression.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "NBA_Salary.html#svr",
    "href": "NBA_Salary.html#svr",
    "title": "NBA Salary Prediction",
    "section": "SVR",
    "text": "SVR\nThe hyperparameters I tuned from SVR are C, gamma and kernel. I needed to tune for Kernel so that my model finds the best hyper plane that fits the datapoints from my target variable. The kernel will deal with the non-linear relationship on between my features and target variable. Tuning for C and gamma will help my model from overfitting.\n\n\nCode\nfrom sklearn.svm import SVR\n\nSVR = SVR()\n\nparam_grid_SVR = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [0.0001, 0.001,0.01, 0.1, 1],\n              'kernel': ['rbf', 'poly']}\ngridSVR= GridSearchCV(SVR, param_grid_SVR, scoring = RMSLE, cv=5)\ngridSVR.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=SVR(),\n             param_grid={'C': [0.1, 1, 10, 100, 1000],\n                         'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'kernel': ['rbf', 'poly']},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridSVR.best_params_)\n\n\nBest parameters: {'C': 0.1, 'gamma': 0.0001, 'kernel': 'poly'}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridSVR.best_score_)\n\n\nTraining error (RMSLE): 1.0135970157480874\n\n\nChecking Validation Error for SVR.\n\n\nCode\nbest_model_SVR = gridSVR.best_estimator_\npredict_y_SVR = best_model_SVR.predict(X_valid_scaled)\n\nSVR_rmsle = mean_squared_log_error(Y_valid, predict_y_SVR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", SVR_rmsle)\n\n\nValidation error (RMSLE): 0.6805718407616901"
  },
  {
    "objectID": "NBA_Salary.html#ridge",
    "href": "NBA_Salary.html#ridge",
    "title": "NBA Salary Prediction",
    "section": "Ridge",
    "text": "Ridge\nFor Ridge Regression, The only hyperparameter I tuned was alpha, which is the penalty term. I chose to search through small, intermediate, and large values of alpha.\n\n\nCode\nridge = Ridge()\nparam_grid_ridge = {'alpha':np.concatenate((np.arange(0.1,2,0.1), np.arange(2, 5, 0.5), np.arange(5, 105, 5)))}\ngridRidge = GridSearchCV(ridge, param_grid_ridge,scoring = RMSLE, cv=5)\ngridRidge.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=Ridge(),\n             param_grid={'alpha': array([  0.1,   0.2,   0.3,   0.4,   0.5,   0.6,   0.7,   0.8,   0.9,\n         1. ,   1.1,   1.2,   1.3,   1.4,   1.5,   1.6,   1.7,   1.8,\n         1.9,   2. ,   2.5,   3. ,   3.5,   4. ,   4.5,   5. ,  10. ,\n        15. ,  20. ,  25. ,  30. ,  35. ,  40. ,  45. ,  50. ,  55. ,\n        60. ,  65. ,  70. ,  75. ,  80. ,  85. ,  90. ,  95. , 100. ])},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridRidge.best_params_)\n\n\nBest parameters: {'alpha': 100.0}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRidge.best_score_)\n\n\nTraining error (RMSLE): 0.5579904916871834\n\n\nChecking Validation Error for Ridge Regression.\n\n\nCode\nbest_model_ridge = gridRidge.best_estimator_\npredict_y_ridge = best_model_ridge.predict(X_valid_scaled)\n\n\nridge_rmsle = mean_squared_log_error(Y_valid, predict_y_ridge,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", ridge_rmsle)\n\n\nValidation error (RMSLE): 0.5200462148165859"
  },
  {
    "objectID": "NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "href": "NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "title": "NBA Salary Prediction",
    "section": "Gradient Boosting Decision Trees Regressor",
    "text": "Gradient Boosting Decision Trees Regressor\nThe hyperparameters I chose to tune for the Gradient Boosting Decision Trees Regressor are n_estimators, learning_rate, and max_depth. GBDTR uses multiple shallow decision trees as weak learners to predict the residuals of the decision trees instead of the target variable. The idea here is use gradient descent to update the residuals after every tree until the model has run through all available trees (n_estimators). Learning rate controls how much the residuals are updated from tree to tree, which can also be described as the “size” of the step in gradient descent. Smaller values of learning rate will allow my model to generalize better on the validation and test data. Max_depth refers to the depth of each tree, which is important because it determines how weak the trees are in the ensemble.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\n\nparam_grid_GBR = {\n    'n_estimators': [100, 200, 300, 400],\n     'learning_rate': [ 0.01, 0.1, 0.2,0.3],\n     'max_depth': [3,4,5,6,7,8]}\n    \n    \n    \ngridGBR= GridSearchCV(GBR, param_grid_GBR,scoring = RMSLE, cv=5)\ngridGBR.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                         'max_depth': [3, 4, 5, 6, 7, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridGBR.best_params_)\n\n\nBest parameters: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridGBR.best_score_)\n\n\nTraining error (RMSLE): 0.7306298089362927\n\n\nChecking Validation Error for Gradient Boosting Decision Trees.\n\n\nCode\nbest_model_GBR = gridGBR.best_estimator_\npredict_y_GBR = best_model_GBR.predict(X_valid)\n\n\nGBR_rmsle = mean_squared_log_error(Y_valid, predict_y_GBR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", GBR_rmsle)\n\n\nValidation error (RMSLE): 0.5708736984050921"
  },
  {
    "objectID": "NBA_Salary.html#final-evaluation-using-test-set",
    "href": "NBA_Salary.html#final-evaluation-using-test-set",
    "title": "NBA Salary Prediction",
    "section": "Final Evaluation using Test Set",
    "text": "Final Evaluation using Test Set\nChecking Test Error for all models. Again I will list the best parameters selected from Grid Search CV.\n\n\nCode\n\nbest_model_RF\n\n\nRandomForestRegressor(max_features=1.0, n_estimators=300)\n\n\nCode\npredict_test_RF = best_model_RF.predict(X_test)\n\n\nRF_test_rmsle = mean_squared_log_error(Y_test, predict_test_RF,squared=False)\n\n\nprint(\"Test error (RMSLE):\", RF_test_rmsle)\n\n\nTest error (RMSLE): 0.5102656876261805\n\n\n\n\nCode\nbest_model_SVR \n\n\nSVR(C=0.1, gamma=0.0001, kernel='poly')\n\n\nCode\npredict_test_SVR = best_model_SVR.predict(X_test_scaled)\n\nSVR_test_rmsle = mean_squared_log_error(Y_test, predict_test_SVR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", SVR_test_rmsle)\n\n\nTest error (RMSLE): 1.0137954511265557\n\n\n\n\nCode\nbest_model_ridge\n\n\nRidge(alpha=100.0)\n\n\nCode\npredict_test_ridge = best_model_ridge.predict(X_test_scaled)\n\n\nridge_test_rmsle = mean_squared_log_error(Y_test, predict_test_ridge,squared=False)\n\n\nprint(\"Test error (RMSLE):\", ridge_test_rmsle)\n\n\nTest error (RMSLE): 0.5376660192539188\n\n\n\n\nCode\n\nbest_model_GBR\n\n\nGradientBoostingRegressor(learning_rate=0.01, max_depth=6)\n\n\nCode\npredict_test_GBR = best_model_GBR.predict(X_test)\n\n\nGBR_test_rmsle = mean_squared_log_error(Y_test, predict_test_GBR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", GBR_test_rmsle)\n\n\nTest error (RMSLE): 0.7888816443032016"
  },
  {
    "objectID": "NJ_604.html",
    "href": "NJ_604.html",
    "title": "Identifying Sources of Poor Nutrition for Americans",
    "section": "",
    "text": "food_density <- read_excel(\"_data/_food/food_density.xlsx\",skip =2 )\n\n\nlookup <- c(Total= \"Total...2\", at_home = \"At home...3\", total_away = \"Total...4\", Restaurant = \"Restaurant...5\", Fast_food = \"Fast food...6\", school  =\"School...7\",other = \"Other...8\", Total1 =\"Total...9\", at_home1 = \"At home...10\", total_away1 = \"Total...11\", Restaurant1 = \"Restaurant...12\",Fast_food1 = \"Fast food...13\",school1  = \"School...14\", other1 =\"Other...15\" )\n\nfood_density<- food_density %>% \n  rename(all_of(lookup)) \n\n\ncolnames(food_density)\n\n [1] \"Food group\"  \"Total\"       \"at_home\"     \"total_away\"  \"Restaurant\" \n [6] \"Fast_food\"   \"school\"      \"other\"       \"Total1\"      \"at_home1\"   \n[11] \"total_away1\" \"Restaurant1\" \"Fast_food1\"  \"school1\"     \"other1\"     \n\n\n\nfood_density<-food_density %>% \n  filter(!row_number() %in% 1)\nfood_density  \n\n\n\n  \n\n\n\n\nfood_density$Restaurant <- as.double(food_density$Restaurant)\nfood_density$Restaurant1 <- as.double(food_density$Restaurant1)\nfood_density$school <- as.double(food_density$school)\nfood_density$school1 <- as.double(food_density$school1)\n\n\nfood_density\n\n\n\n  \n\n\n\n\nfood_density %>% \n  pivot_longer(c(\"Total\" ,\"at_home\", \"total_away\",\"Restaurant\", \"Fast_food\", \"school\" ,     \"other\" ,\"Total1\"  ,\"at_home1\" ,\"total_away1\", \"Restaurant1\", \"Fast_food1\", \"school1\" ,\"other1\" ), names_to = \"food_source\", values_to = \"values\")\n\n\n\n  \n\n\n\n\nfood_density<- food_density %>% \n  mutate(food_type = \"Added_sugar\")\n\n\nx0<- food_density %>% slice(2:8)\n\n\nx1<- food_density %>% \n    slice(10:16) %>% \n    mutate(food_type = \"Discretionary_fats\" )\n  \nx2<-food_density %>% \n    slice(18:24) %>% \n    mutate(food_type = \"Discretionary_oils\" )\n\nx3<-food_density %>% \n    slice(26:32) %>% \n    mutate(food_type = \"Dairy\")\n\nx4<-food_density %>% \n    slice(34:40) %>% \n    mutate(food_type = \"Fruit\" )\n\nx5<-food_density %>% \n    slice(42:48) %>% \n    mutate(food_type = \"Vegetables_total\" )\n\nx6<-food_density %>% \n    slice(50:56) %>% \n    mutate(food_type = \"Potatoes\" )\n\nx7<-food_density %>% \n    slice(58:64) %>% \n    mutate(food_type = \"Tomatoes\" )\n\nx8<-food_density %>% \n    slice(66:72) %>% \n    mutate(food_type = \"Red_Orange_Vegatables\" )\n\nx9<-food_density %>% \n    slice(74:80) %>% \n    mutate(food_type = \"Dark_Green_Vegatables\" )\n\nx10<-food_density %>% \n    slice(82:88) %>% \n    mutate(food_type = \"Grains\" )\n\nx12<-food_density %>% \n    slice(90:96) %>% \n    mutate(food_type = \"Grains_Refined\")\n\nx13<-food_density %>% \n    slice(98:104) %>% \n    mutate(food_type = \"Grains_Whole\" )\n\nx14<-food_density %>% \n    slice(106:112) %>% \n    mutate(food_type = \"Protein_foods\" )\n\n\nfood_density<- full_join(x0,x1) %>%\n  full_join(x2) %>%\n  full_join(x3) %>%\n  full_join(x4) %>%\n  full_join(x5) %>% \n  full_join(x6) %>% \n  full_join(x7) %>% \n  full_join(x8) %>% \n  full_join(x9) %>% \n  full_join(x10) %>% \n  full_join(x12) %>% \n  full_join(x13) %>%\n  full_join(x14)\n\n\nfood_density <- food_density %>% relocate(food_type, .after = `Food group`)\n\n\nfood_density <-food_density %>% \n  pivot_longer(c(\"Total\" ,\"at_home\", \"total_away\",\"Restaurant\", \"Fast_food\", \"school\" ,     \"other\" ,\"Total1\"  ,\"at_home1\" ,\"total_away1\", \"Restaurant1\", \"Fast_food1\", \"school1\" ,\"other1\" ), names_to = \"food_source\", values_to = \"values\")\n\n\nz<- \"2016-2017\"\nfood_density<-food_density %>% \n              mutate(year = case_when(\n              food_source == \"Total1\" ~ \"2017-2018\",\n              food_source == \"at_home1\" ~ \"2017-2018\",\n              food_source == \"total_away1\"~ \"2017-2018\",\n              food_source == \"Restaurant1\"~ \"2017-2018\",\n              food_source == \"Fast_food1\"~ \"2017-2018\",\n              food_source == \"school1\"~ \"2017-2018\",\n              food_source == \"other1\"~ \"2017-2018\",\n              .default = z)) %>% \n              relocate(year, .after = `Food group`)\n\n\nfood_density\n\n\n\n  \n\n\n\n\nfood_density <-food_density %>% rename(\"Demographics\"= `Food group`)\n\n\nfood_density<-food_density %>% \n  mutate(food_source = str_remove(food_source, \"1\"))\n\n\nfood_density <- food_density %>% \n  mutate(Demographics = str_remove(Demographics, \"1$\"))\nfood_density <- food_density %>% \n  mutate(Demographics = str_remove(Demographics, \"2$\"))\n\n\nfood_density\n\n\n\n  \n\n\n\n\nfood_benchmark <- read_excel(\"_data/_food/food_benchmark.xlsx\",skip =2)\n\n\nfood_benchmark<- food_benchmark %>%\n  filter(!row_number() %in% c(1, 2, 13,14,15))\n\n\nfood_benchmark$`1400.0`<- as.double(food_benchmark$`1400.0`)\n\n\nfood_benchmark <- food_benchmark %>% pivot_longer(c(\"1400.0\", \"2000.0\", \"2200.0\",\"3000.0\" ), names_to = \"Calorie_Intake_Level\", values_to = \"values\")\n\n\nfood_benchmark\n\n\n\n  \n\n\n\n\nfood_nutrient_density <- read_excel(\"_data/_food/food_nutrient_density.xlsx\",skip =2 )\n\n\nfood_nutrient_density<- food_nutrient_density  %>% \n                        rename(all_of(lookup)) \n\n\nfood_nutrient_density<-food_nutrient_density %>% \n  filter(!row_number() %in% 1)\n\n\nfood_nutrient_density$Restaurant <- as.double(food_nutrient_density$Restaurant)\nfood_nutrient_density$Restaurant1 <- as.double(food_nutrient_density$Restaurant1)\nfood_nutrient_density$school <- as.double(food_nutrient_density$school)\nfood_nutrient_density$school1 <- as.double(food_nutrient_density$school1)\n\n\nfood_nutrient_density<- food_nutrient_density %>% \n  mutate(nutrient_type = \"Calcium\")\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\nx0<- food_nutrient_density %>% slice(2:8)\n\nx1<- food_nutrient_density %>% \n    slice(10:16) %>% \n    mutate(nutrient_type = \"Cholesterol\")\n  \nx2<-food_nutrient_density %>% \n    slice(18:24) %>% \n    mutate(nutrient_type = \"Fiber\")\n\nx3<-food_nutrient_density %>% \n    slice(26:32) %>% \n    mutate(nutrient_type = \"Iron\")\n\nx4<-food_nutrient_density %>% \n    slice(34:40) %>% \n    mutate(nutrient_type = \"Saturated_fat\" )\n\nx5<-food_nutrient_density %>% \n    slice(42:48) %>% \n    mutate(nutrient_type = \"Total_fat\" )\n\nx6<-food_nutrient_density %>% \n    slice(50:56) %>% \n    mutate(nutrient_type = \"Sodium\" )\n\n\nfood_nutrient_density<- full_join(x0,x1) %>%\n  full_join(x2) %>%\n  full_join(x3) %>%\n  full_join(x4) %>%\n  full_join(x5) %>% \n  full_join(x6)\n\n\nfood_nutrient_density <- food_nutrient_density %>% relocate(nutrient_type, .after = `Nutrient group`)\n\n\nfood_nutrient_density<- food_nutrient_density %>% \n  pivot_longer(c(\"Total\" ,\"at_home\", \"total_away\",\"Restaurant\", \"Fast_food\", \"school\" ,     \"other\" ,\"Total1\"  ,\"at_home1\" ,\"total_away1\", \"Restaurant1\", \"Fast_food1\", \"school1\" ,\"other1\" ), names_to = \"nutrient_source\", values_to = \"values\")\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\nfood_nutrient_density<-food_nutrient_density %>% \n              mutate(year = case_when(\n              nutrient_source == \"Total1\" ~ \"2017-2018\",\n              nutrient_source == \"at_home1\" ~ \"2017-2018\",\n              nutrient_source == \"total_away1\"~ \"2017-2018\",\n              nutrient_source == \"Restaurant1\"~ \"2017-2018\",\n              nutrient_source == \"Fast_food1\"~ \"2017-2018\",\n              nutrient_source == \"school1\"~ \"2017-2018\",\n              nutrient_source == \"other1\"~ \"2017-2018\",\n              .default = z)) %>% \n              relocate(year, .after = `Nutrient group`)\n\n\nfood_nutrient_density <-food_nutrient_density %>% rename(\"Demographics\"= `Nutrient group`)\n\nfood_nutrient_density<-food_nutrient_density %>% \n  mutate(nutrient_source = str_remove(nutrient_source, \"1\"))\n\nfood_nutrient_density <- food_nutrient_density %>% \n  mutate(Demographics = str_remove(Demographics, \"1$\"))\nfood_nutrient_density <- food_nutrient_density %>% \n  mutate(Demographics = str_remove(Demographics, \"2$\"))\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\nfood_density\n\n\n\n  \n\n\nfood_benchmark\n\n\n\n  \n\n\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Calcium\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\")\n             , y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Calcium Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Calcium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nOut of all food sources, Calcium density (the average amount of nutrient for each 1,000 calories in a person’s diet) was highest in schools for 2 out of the 3 income levels (Low and Middle Income). For example, for each 1,000 calories consumed, Low and Middle income individuals consumed an average of 826.8mg and 904.09 mg of calcium at school daily. However, Calcium density for High income individuals was highest when eating at other away-from-home places. For each 1,000 calories consumed, High Income Individuals consumed an average of 760.69 mg of Calcium at other away-from-home places daily.\ncompare magnitudes between highest and 2nd highest calcium density level (show that low income has largest gap in calcium densities )\nLow Income individuals consume a daily average of 597.13mg of calcium per 1,000 calories at Home, which is higher than amounts consumed at home for Middle Income (514.79mg) and High Income (571.69mg) individuals. For Food away from home excluding school (Fast Food, Restaurant, other), Low Income individuals have the lowest Calcium density in each of those categories compared to Middle and High Income individuals. For example, Low Income individuals consumed a daily average of 351.11 mg of calcium per 1,000 calories at other away-from-home places, which is much lower compared to Middle Income (838.37 mg) and High Income (760.69 mg).\nLow Income individuals have the highest Calcium density at Home compared to Middle and High Income individuals. For Food away from home excluding school (Fast Food, Restaurant, other) Low Income individuals have the lowest Calcium density in each of those categories compared to Middle and High Income individuals.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Saturated_fat\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Saturated Fat Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Saturated Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, all age groups had higher daily average intakes of Saturated Fat at away from home food sources (Total) compared to at home food sources.In addition, Saturated Fat density at Fast Food restaurants ranked the highest for each Age group at out of all food sources. Adults consume 2.49 more grams of saturated fat per 1,000 calories at Fast Food restaurants compared to at home, followed by Seniors (2.37g), and Children (2.24g).\nFor Adults and Seniors, Saturated Fat density at Home ranked the lowest compared to all food sources away from home. For Children However, Saturated Fat density at school ranked the lowest compared to at home food sources and all other food sources away from Home.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_fats\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Fat Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Discretionary Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source:2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nFor all age groups, Discretionary Fat density was lower at Home compared to away from Home. However, the average amount of daily Discretionary Fat per 1,00 calories in ones diet for away from home food sources excluding school (Fast Food, Restaurant, other) rank differently for each age group. For example, other away-from-home places, Fast food, and Restaurants rank 1st, 2nd and 3rd in Discretionary Fat density for Adults and Seniors, respectfully. On the contrary, Fast food ranked 1st other away-from-home places ranked 2nd and Restaurants ranked 3rd in Discretionary Fat density for Children (density values for Fast food, Restaurants and other away from home food sources are 18.4 g/1,000kcal, 17.92 g/1,000kcal and 16.23g/kcal, respectfully).\nThe gap between away-from-home places Discretionary Fat density and At home Discretionary Fat density is much higher for Seniors than for Children and Adults. Among Seniors, other away-from-home places Discretionary Fat density is higher than At home Discretionary Fat density by 4.01 g/1,000kcal. Among Children and Adults, other away-from-home places Discretionary Fat density is higher than At home Discretionary Fat density 1.39 g/1,000kcal and by 3.03 g/1,000kcal, respectfully.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_oils\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Oil Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Discretionary Oils (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nFor all age groups, Discretionary Oils Density was highest when people consumed food at Restaurants compared to at home and other away from home food sources (Fast Food, Other). For example, for each 1,000 calories, Adults consume a daily average of 19.69g of Discretionary oils when eating at Restaurants, which is a higher amount compared to at home (11.79g), at Fast Food establishments (15.05g), and at other away-from-home places (10.5g).\nMore specifically, Children have the largest difference in Discretionary Oils Density between at home and at Fast food restaurants compared to Adults and seniors. For example, the difference in Discretionary Oils Density between at home and at Fast food restaurants for Children is 4.64g per 1000 calories, which is a larger difference than adults (3.28g/1000kcal) and Seniors (1.26/1000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Added_sugar\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Added Sugar Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Added Sugar (tsp/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, Added sugar density was higher when individuals consumed food at other away-from-home sources compared to at home food sources for all age groups. However, the difference in Added sugar density between other away-from-home and at home food sources is largest for children compared to Adults and Seniors. Among Children, the difference in Added sugar density between at home and other away-from-home sources is 5.59tsp per 1000 calories, which is a larger difference than Adults (2.37tsp/1000kcal) and Seniors (1.98tsp/1000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Dairy\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Dairy Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Dairy (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn this figure, Seniors have the largest difference in Dairy Density between at home and total away from home food sources compared to Adults and Children. For example, for each 1,000 calories, Seniors consume a daily average of 0.25 less cups of Dairy when eating away from home compared to at home. Children are consuming more dairy per 1,000 calories at school compared to all other food sources\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Fruit\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fruit Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Fruit (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, Adults consume less fruit per 1000 calories at home than Children and Seniors. For each 1,000 calories, Adults consumed a daily average of 0.59 cups of Fruit at home, which is 0.16 and 0.32 cups less than the amount consumed by Seniors (0.75 cups) and Children (0.91 cups), respectively. Adults and Seniors Daily average intakes of Fruit per 1,000 calories at home are over 3 times more than away from home. Children are consuming more Fruit per 1,000 calories at school compared to all other food sources.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Vegetables_total\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Vegetable Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Vegetable (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nFor each 1,000 calories, children consume a daily average of 0.47 cups of Vegetables at home, which is lower than Adults (0.68 cups) and Seniors (0.74 cups).\nFor all age groups, Total Vegetable Density was highest when people consumed food at Restaurants compared to at home and other away from home food sources (Fast Food, Other). In addition, Children consumed 0.78 cups of vegetables per 1000 calories, which is 0.36 cups/1000kcal and 0.42 cups/1000kcal less than Adults (1.14 cups/1000kcal) and Seniors (1.23 cups/1000kcal), respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Grains\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Grain Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Grains (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for each 1000 calories, the daily average intake of total grains is smaller when the food source is from Home compared to total away from home for all age groups. Adults have the largest difference in the daily average intake of total grains between at home and total away from home sources and lowest at home grain intake . For example, for every 1000 calories, Adults consume 2.79oz of grains at home, which is 0.37 oz/1000kcal less than the amount they consume away from home.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Protein_foods\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Protein Foods Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Protien Foods (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average intake of proteins foods was highest at Restaurants compared to at home and and other away from home food sources (Fast Food, Other) for all age groups. For example, Adults consumes a daily average of 4.2 ounces of Protein Foods per 1,000 calories at restaurants, 1.48oz higher than the amount consumed at Home (2.72 oz/1000kcal).\nChildren are consuming about the same amount of Protein foods per 1,000 calories at home (2.12 oz/1000kcal) and at away from home (2.11 oz/1000kcal), whereas Adults and Seniors are consuming more Protein foods away from home compared to at home. In addition, for every 1,000 calories, Seniors and Adults are consuming 0.38 and 0.28 more ounces of protein foods away from home than at home, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Protein_foods\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Protein Foods Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Protien Foods (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nTypically, Low Income and Middle income individuals are consuming more Protein foods per 1,000 calories at Fast Food establishments and restaurants compared to at home. For example, for every 1,000 calories, Low income individuals consume a daily average of 4.1 and 2.7 ounces of Protein Foods at restaurants and Fast food establishments, respectively, which is more than what they consume at home (2.45oz). However, High Income individuals consume more protein foods per 1,000 calories at home (2.73 oz) compared to Fast food establishments (2.26 0z), but not Restaurants (4.25 oz).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Grains\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Grain Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Grains (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average intake of Grains is smaller when the food source is from Home compared to total away from home for all levels of income. Specifically, High income individuals have the largest difference in Grain density, as they consume 0.45 less ounces of Grains per 1,000 calories at home than they do away from home ( Grain densities At home and away from home are 2.83 oz/1000kcal and 3.28 oz/1000kcal, respectively).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Vegetables_total\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Vegetable Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Vegetable (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average consumption of Vegetables was higher when the food source is from Restaurants compared to at home and other away from home food sources (Fast Food, Other). In addition the daily average consumption of Vegetables is almost the same at home compared to the total away from home for all income levels. The difference in vegetable densities between at home and total away from home are all lower than 0.15 cups/1000kcal for low, middle and high incomes.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Fruit\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fruit Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Fruit (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, low income individuals have the highest daily average consumption of Fruit away from home, which is 0.16 cups higher than middle income (0.26cup/1000kcal) and double the amount consumed by high income individuals (0.19 cup/1000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Dairy\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Dairy Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Dairy (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, there is little difference between the daily average amount of dairy intake per 1,000 calories at home and away from home food sources for all income levels. More specifically, there is a less than 0.1 cup/1000kcal difference between at home and away from home food sources for all income levels.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Added_sugar\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Added Sugar Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Added Sugar (tsp/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average consumption of added sugars was higher when the food source is from Home compared to total away from home for all income levels. However, other away-from-home sources (street vendors, vending machines, etc.) had the highest added sugar density compared to Fast Food establishments and Restaurants for all income levels. For example, the daily average consumption of added sugars for low income individuals at other away-from-home sources was 12.46 teaspoons per 1,000 calories, which is higher than the amount consumed at Fast Food establishments (7.46 tsp/1,000kcal) and Restaurants (4.84 tsp/1,000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_oils\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Oil Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Discretionary Oil (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average consumption of Discretionary oil was higher when the food sources is away from home compared to at home for all income levels. In addition, restaurants provide the highest intake of daily Discretionary oils, followed by Fast food establishments and then other away-from-home food sources. For example, High income individuals consumed a daily average of 20 grams per 1,000 calories when eating at Restaurants, which is higher than what they consume at Fast food establishments (14.29 grams) and at other away-from-home food sources (10.9 grams).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_fats\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Fat Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Discretionary Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, other away from home food sources had the highest daily average intake of Discretionary fat compared to all other away from home food sources (Fast Food ,Restaurant) for 2 of the 3 income levels. However, for Low income individuals, Fast Food establishments had the highest daily average intake of Discretionary fat per 1,000 calories (17.98 grams) compared to all other away from home food sources (densities for other away from home food sources and Restaurants are 16.76g and 14.91g, respectively)\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Cholesterol\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Cholesterol Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Cholesterol (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, High income individuals have a higher Cholesterol density when consuming food away from home compared to middle and low income individuals. In addition, for ever 1,000 calories, High income indivuals consumed a daily average of 153.21 mg of Cholesterol away from home, which is 25.47mg and 21.99mg higher than Middle and Low income individuals, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Fiber\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fiber Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Fiber (g/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, at home food sources had the highest daily average intake of Fiber compared to away from home food sources for all income levels. However, High income individuals had the highest difference in daily fiber consumption per 1,000 calories between at home and away from home food sources, followed by Middle income and low income individuals. For example, the difference in the daily average fiber intake per 1,000 calories for High income individuals between at home and away at home food sources is 2.1 grams, which is higher than the difference for Middle (1.46 grams) and Low income Individuals (0.93 grams).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Iron\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Iron Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Iron (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, at home food sources had the highest daily average intake of Iron compared to away from home food sources for all income levels.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Saturated_fat\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Saturated Fat Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Saturated Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, the daily average consumption of Saturated Fat per 1,000 calories was highest at Fast Food establishments compared to at home, restaurants, and other away-from-home food sources for all levels of income. In addition, Low income individuals had the largest difference in daily Saturated Fat consumption per 1,000 calories between at home and Fast food sources, followed by Middle income and High income individuals. For example, the daily average Saturated fat intake per 1,000 calories for Low income individuals at Fast Food establishments is 2.86 grams higher than at home, which is larger than the difference for Middle (2.34 grams) and High income Individuals (1.92 grams). However, High income individuals had the largest difference in daily Saturated Fat consumption per 1,000 calories between at home and other away-from-home food sources, consuming 1.57 g/1,0000kcal more at Fast food establishments compared to at home.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Total_fat\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Fat Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Total Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Total Fat compared to at home food sources for all income levels. In addition, the daily average Total fat intake per 1,000 calories for Middle income individuals away from home is 5.57 grams higher than at home, which is larger than the difference for Low (5.41 grams) and High income Individuals (4.97 grams).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Sodium\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Sodium Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Sodium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Sodium compared to at home food sources for all income levels.In addition, the daily average Sodium intake per 1,000 calories for High income individuals away from home is 572.92 mgs higher than at home, which is larger than the difference for Middle (445.61 mg) and Low income Individuals (323.87 grams). Looking specifically at other away from home food sources, High income individuals consumed a daily average of 2386.09 mg per 1,000 calories, which is 387 mg and 685.55 mg more than Middle and low income individuals, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Calcium\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Calcium Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Calcium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, At Home food sources had the highest daily average intake of calcium compared to all away from home food sources (Fast Food ,Restaurant, Other) for 2 of the 3 age groups. However, Adults had the highest daily average intake of calcium when eating at other away from home food sources (811.39 mg/1,000 calories) compared to At home (584.18 mg/1,000 calories), Fast Food establishments (490.21 mg/1,000 calories), and Restaurants (347.94 mg/1,000 calories). Seniors have the lowest daily average intake of calcium per 1,000 calories at home (506.7 mg), which is 77.48 mg and 83.13mg less than Adults and Children, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Cholesterol\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Cholesterol Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Cholesterol (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, Adults and Seniors had higher daily average intakes of Cholesterol at away from home food sources compared to at home food sources. In addition, the daily average Cholesterol intake per 1,000 calories for Seniors eating away from home is 165.5 mg, which is 16.49 mg higher than the the daily average intake for Adults (149.01 mg). However, Children have a lower daily average intake of Cholesterol at away from home food sources compared to at home food sources. Out of all away from home food sources (Fast Food ,Restaurant, Other), Restaurants have the highest daily average intake of Cholesterol for all age groups.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Fiber\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fiber Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Fiber (g/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, at home food sources had the highest daily average intake of Fiber compared to away from home food sources for all age groups. In addition, the daily average Fiber intake per 1,000 calories for Seniors eating away from home is 2.58 grams less than what they intake at home, which is a larger difference than Adults (1.55g, 8.21 at home, 6.66 away from home) and Children (0.82g, 8.17g at home, 7.37 away from home).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Iron\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Iron Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Iron (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, at home food sources had the highest daily average intake of Iron compared to away from home food sources for all age groups. In addition, the daily average Iron intake per 1,000 calories for Children eating away from home is 2.08 mg less than what they intake at home, which is a larger difference than Adults (1.26 mg, 7.09 mg at home, 5.83 mg away from home) and Seniors (1.39 mg, 7.78 mg at home, 6.39 mg away from home).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Total_fat\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Fat Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Total Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Total compared to at home food sources for all age groups. In addition, the daily average Fat intake per 1,000 calories for Adults eating away from home was 6.18 g more than what they comsume at home, which is a larger difference than Children (2.95 g, 36.15 g at home, 39.4 g away from home) and Seniors (5.37 g, 44.21 at home, 38.84g away from home). However, the daily average Fat intake per 1,000 calories for Children eating at Restaurants (44.04 g) and Fast food (43.02 g) establishment is 7.89 g and 6.87 g more than what they consume at home, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Sodium\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Sodium Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Sodium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Sodium compared to at home food sources for all age groups. However, Children and Seniors are consuming more Sodium at Restaurants compared to at home, Whereas Adults are consuming more sodium at Other away-from-home food sources. For example, the daily average Sodium intakes per 1,000 calories for Children and Seniors eating at restaurants were 617.9 mg and 118.14 more than what they consume at home respectively, Whereas Adults eating at Other away-from-home places consume 936.16 more grams of Sodium than they do at home."
  },
  {
    "objectID": "NJ_Adv_Quant.html",
    "href": "NJ_Adv_Quant.html",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "",
    "text": "Code\nGSS_18<- read_dta(\"_data/GSS2018.dta\")\n\nGSS_22<- read_dta(\"_data/GSS2022.dta\")"
  },
  {
    "objectID": "NJ_Adv_Quant.html#selecting-variables-to-use",
    "href": "NJ_Adv_Quant.html#selecting-variables-to-use",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Selecting variables to use",
    "text": "Selecting variables to use\nHere I renamed the different weight columns to ‘weights’ so that I could combine the datasets\n\n\n\nCombine datasets using rbind() and named it GSS\n\n\nCode\nGSS <-rbind(GSS_18,GSS_22)"
  },
  {
    "objectID": "NJ_Adv_Quant.html#recoding-variables-and-removing-nas",
    "href": "NJ_Adv_Quant.html#recoding-variables-and-removing-nas",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Recoding variables and removing NAs",
    "text": "Recoding variables and removing NAs\nHere I recoded all my variables with new names. I flipped the scale of satjob1 so that the negative responses were coded as lower values and the positive responses were coded as higher values. Age_group, years_job, POC, and use_tech all had to be collapsed into smaller groups in order to run analysis. All binary variables were coded as 0/1. Work_type was coded as a categorical variable.\n\n\nCode\nGSS %>%                  \n  count(satjob1) %>%                             \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 5 × 3\n  satjob1                          n percent\n  <dbl+lbl>                    <int> <chr>  \n1     1 [very satisfied]        1616 27.4%  \n2     2 [somewhat satisfied]    1406 23.9%  \n3     3 [not too satisfied]      256 4.3%   \n4     4 [not at all satisfied]    82 1.4%   \n5 NA(i) [iap]                   2532 43.0%  \n\n\nCode\nGSS <- GSS %>% \n  mutate(job_sat = case_when(\n    satjob1 ==1 ~ 4,\n    satjob1 ==2 ~ 3,\n    satjob1 ==3 ~ 2,\n    satjob1 ==4 ~ 1,\n    satjob1 < 0 ~ NA_real_))\n\nGSS %>%                \n  count(job_sat) %>%                            \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 5 × 3\n  job_sat     n percent\n    <dbl> <int> <chr>  \n1       1    82 1.4%   \n2       2   256 4.3%   \n3       3  1406 23.9%  \n4       4  1616 27.4%  \n5      NA  2532 43.0%  \n\n\nCode\nGSS <- GSS %>% \n  mutate(wrkhome = case_when(\n    wrkhome ==1 ~ 1,\n    wrkhome ==2 ~ 2,\n    wrkhome ==3 ~ 3,\n    wrkhome ==4 ~ 4,\n    wrkhome ==5 ~ 5,\n    wrkhome ==6 ~ 6,\n    wrkhome < 0 ~ NA_real_))\n\nGSS <- GSS %>% \n  mutate(degree = case_when(\n    degree ==0 ~ 1,\n    degree ==1 ~ 2,\n    degree ==2 ~ 3,\n    degree ==3 ~ 4,\n    degree ==4 ~ 5,\n    degree < 0 ~ NA_real_))\n\n\nGSS <- GSS %>% \n  mutate(post_covid = case_when(\n    year ==2018 ~ 0,\n    year ==2022 ~ 1,\n    year < 0 ~ NA_real_))\n\n\n\nGSS<- GSS %>%\n  mutate(use_tech = case_when(\n    usetech >= 0 & usetech <= 25 ~ 1,\n    usetech > 25 & usetech <= 50 ~ 2,\n    usetech > 50 & usetech <= 75 ~ 3,\n    usetech > 75 ~ 4,\n  ))\n\nGSS %>%               \n  count(age) %>%                           \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 73 × 3\n   age           n percent\n   <dbl+lbl> <int> <chr>  \n 1 18           44 0.747% \n 2 19           55 0.933% \n 3 20           63 1.069% \n 4 21           73 1.239% \n 5 22           86 1.460% \n 6 23           82 1.392% \n 7 24           83 1.409% \n 8 25           88 1.494% \n 9 26           89 1.511% \n10 27           85 1.443% \n# … with 63 more rows\n\n\nCode\nGSS <- GSS %>%\n  mutate(age = replace(age, age <= -1, NA))\n\nage_breaks <- c(18, 30, 40, 50, 65, 100)\n\n# Create a new variable with age groups\nGSS$age_group <- cut(GSS$age, \n                      age_breaks, labels = c(1, 2, 3, 4, 5), \n                      include.lowest = TRUE)\n\n\n\nGSS <- GSS %>% \n  mutate(female = case_when(\n    sex ==1 ~ 0,\n    sex ==2 ~ 1,\n    sex <=-1 ~ NA_real_)) \n\nGSS %>%                 \n  count(wrktype) %>%                             \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 6 × 3\n  wrktype                                                        n percent\n  <dbl+lbl>                                                  <int> <chr>  \n1     1 [independent contractor/consultant/freelance worker]   434 7.37%  \n2     2 [on-call, work only when called to work]                82 1.39%  \n3     3 [paid by a temporary agency]                            35 0.59%  \n4     4 [work for contractor who provides workers/services]    115 1.95%  \n5     5 [regular, permanent employee]                         2704 45.89% \n6 NA(i) [iap]                                                 2522 42.80% \n\n\nCode\nGSS <- GSS %>%\n  mutate(work_type = replace(wrktype, (wrktype <= -1 | wrktype >= 99), NA))\nfreq(GSS$work_type)\n\n\n\n\n\nwork arrangement at main job \n      Frequency Percent Valid Percent\n1           434   7.366        12.878\n2            82   1.392         2.433\n3            35   0.594         1.039\n4           115   1.952         3.412\n5          2704  45.893        80.237\nNA's       2522  42.804              \nTotal      5892 100.000       100.000\n\n\nCode\nvalue_labels <- c(\"Independent contractor/consultant/freelance worker\", \"On-call, work only when called to work\", \n                  \"Paid by a temporary agency\", \"Work for contractor who provides workers/services\",\n                  \"Regular, permanent employee\") \n\n\n# Assign value labels to the Response variable\nGSS$work_type <- factor(GSS$work_type, levels = 1:5, labels = value_labels)\n\n\nGSS %>%                \n  count(race) %>%                            \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 4 × 3\n  race              n percent\n  <dbl+lbl>     <int> <chr>  \n1     1 [white]  4207 71.4%  \n2     2 [black]   950 16.1%  \n3     3 [other]   682 11.6%  \n4 NA(i) [iap]      53 0.9%   \n\n\nCode\nGSS <- GSS %>% \n  mutate(poc = case_when(\n    race==1 ~ 0,\n    race == 2 | race == 3 ~ 1,\n    race <=-1 ~ NA_real_)) \n\n\n\n\nGSS<- GSS %>%\n  mutate(years_job = case_when(\n    yearsjob >= 0 & yearsjob <= 1.0 ~ 1,\n    yearsjob > 1.0 & yearsjob <= 4 ~ 2,\n    yearsjob > 4 ~ 3,\n  ))\n\n\n\nGSS %>%                 \n  count(years_job) %>%                             \n  mutate(percent = scales::percent(n / sum(n)))\n\n\n# A tibble: 4 × 3\n  years_job     n percent\n      <dbl> <int> <chr>  \n1         1  1017 17.3%  \n2         2   802 13.6%  \n3         3  1556 26.4%  \n4        NA  2517 42.7%"
  },
  {
    "objectID": "NJ_Adv_Quant.html#dv-distribution",
    "href": "NJ_Adv_Quant.html#dv-distribution",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "DV distribution",
    "text": "DV distribution\nHere is a graph of the distribution of my DV.\n\n\nCode\ncustom_order <- c('Not at all satisfied', 'Not too satisfied', 'Somewhat satisfied', 'Very satisfied')\n\nggplot(GSS, aes(x = job_sat)) +\n  geom_density(fill = \"blue\", alpha = 0.5) +\n  labs(x = \"Value\", y = \"Density\", title = \"Distribution of Job Satisfaction\")  + \n  scale_x_discrete(limits = custom_order)\n\n\nWarning: Removed 2532 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\nGraph suggested that an ordered model would be most appropriate for my models."
  },
  {
    "objectID": "NJ_Adv_Quant.html#check-to-see-if-there-is-a-nested-data-structure-based-on-the-grouping-variable",
    "href": "NJ_Adv_Quant.html#check-to-see-if-there-is-a-nested-data-structure-based-on-the-grouping-variable",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Check to see if there is a nested data structure based on the grouping variable",
    "text": "Check to see if there is a nested data structure based on the grouping variable\nWork type is a grouping variable, so I checked to see if there is a nested data structure.\n\n\nCode\nGSS$wrktype <- as.character(GSS$wrktype)\n\nmlm <- lmer(job_sat ~ 1 + (1|wrktype), data = GSS, weights = weights)\nsummary(mlm)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: job_sat ~ 1 + (1 | wrktype)\n   Data: GSS\nWeights: weights\n\nREML criterion at convergence: 8212\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-12.1296  -0.4674  -0.2270   0.7235   2.7186 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n wrktype  (Intercept) 0.02348  0.1532  \n Residual             0.52486  0.7245  \nNumber of obs: 3321, groups:  wrktype, 5\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  3.39041    0.07508   45.16\n\n\nCode\nicc <- 0.02348/(0.02348 +0.52486)\n\nprint(icc)\n\n\n[1] 0.04282015\n\n\nICC < .1 indicates that there is not multilevel data structure within my dataset."
  },
  {
    "objectID": "NJ_Adv_Quant.html#changing-the-data-types-for-important-variables",
    "href": "NJ_Adv_Quant.html#changing-the-data-types-for-important-variables",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Changing the data types for important variables",
    "text": "Changing the data types for important variables\nFirst, job_sat must be set as an ordered factor so that ordered logistic regression can be run. post_covid has to be set as a factor in order to run the interaction model. Use_tech_f and wrkhome_f are factor versions of use_tech and wrkhome. I set them to factors to investigate if they have a linear relationship with job satisfaction. Age_group is set as a numeric because it is a control variable. Finally I filter out any NAs again to ensure that all my data instances are clean.\n\n\nCode\nGSS$job_sat <- factor(GSS$job_sat, ordered = TRUE)\nGSS$post_covid <- factor(GSS$post_covid)\nGSS$use_tech_f <- factor(GSS$use_tech)\nGSS$wrkhome_f <- factor(GSS$wrkhome)\nGSS$age_group <- as.numeric(GSS$age_group)\n\nGSS <- GSS %>% \n  filter(complete.cases(.))"
  },
  {
    "objectID": "NJ_Adv_Quant.html#check-to-see-if-use_tech-is-linear",
    "href": "NJ_Adv_Quant.html#check-to-see-if-use_tech-is-linear",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Check to see if use_tech is linear",
    "text": "Check to see if use_tech is linear\n\n\nCode\njs_l<-polr(job_sat ~ use_tech_f + wrkhome + post_covid + female + poc + age_group + work_type + years_job, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\nstargazer(js_l, type=\"text\",\n          digits=2)\n\n\n\n======================================================================================\n                                                               Dependent variable:    \n                                                           ---------------------------\n                                                                     job_sat          \n--------------------------------------------------------------------------------------\nuse_tech_f2                                                          -0.17*           \n                                                                     (0.10)           \n                                                                                      \nuse_tech_f3                                                           0.01            \n                                                                     (0.12)           \n                                                                                      \nuse_tech_f4                                                         -0.37***          \n                                                                     (0.09)           \n                                                                                      \nwrkhome                                                              0.07***          \n                                                                     (0.02)           \n                                                                                      \npost_covid1                                                           -0.06           \n                                                                     (0.07)           \n                                                                                      \nfemale                                                                -0.05           \n                                                                     (0.07)           \n                                                                                      \npoc                                                                  -0.15*           \n                                                                     (0.08)           \n                                                                                      \nage_group                                                            0.22***          \n                                                                     (0.03)           \n                                                                                      \nwork_typeOn-call, work only when called to work                       -0.20           \n                                                                     (0.24)           \n                                                                                      \nwork_typePaid by a temporary agency                                 -1.63***          \n                                                                     (0.36)           \n                                                                                      \nwork_typeWork for contractor who provides workers/services            -0.33           \n                                                                     (0.22)           \n                                                                                      \nwork_typeRegular, permanent employee                                -0.55***          \n                                                                     (0.11)           \n                                                                                      \nyears_job                                                            -0.09**          \n                                                                     (0.04)           \n                                                                                      \n--------------------------------------------------------------------------------------\nObservations                                                          3,268           \n======================================================================================\nNote:                                                      *p<0.1; **p<0.05; ***p<0.01\n\n\nThe non linear relationship suggests that use_tech should be coded as a factor."
  },
  {
    "objectID": "NJ_Adv_Quant.html#check-to-see-if-wrkhome-is-linear",
    "href": "NJ_Adv_Quant.html#check-to-see-if-wrkhome-is-linear",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Check to see if wrkhome is linear",
    "text": "Check to see if wrkhome is linear\nNon linear relationship suggests that wrkhome should be coded as a factor.\n\n\nCode\njs_l<-polr(job_sat ~ use_tech + wrkhome_f + post_covid + female + poc + age_group + work_type + years_job, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\nstargazer(js_l, type=\"text\",\n         digits=2)\n\n\n\n======================================================================================\n                                                               Dependent variable:    \n                                                           ---------------------------\n                                                                     job_sat          \n--------------------------------------------------------------------------------------\nuse_tech                                                            -0.12***          \n                                                                     (0.03)           \n                                                                                      \nwrkhome_f2                                                            0.13            \n                                                                     (0.14)           \n                                                                                      \nwrkhome_f3                                                           0.54***          \n                                                                     (0.16)           \n                                                                                      \nwrkhome_f4                                                            0.26*           \n                                                                     (0.14)           \n                                                                                      \nwrkhome_f5                                                           0.27**           \n                                                                     (0.11)           \n                                                                                      \nwrkhome_f6                                                           0.33***          \n                                                                     (0.12)           \n                                                                                      \npost_covid1                                                           -0.05           \n                                                                     (0.07)           \n                                                                                      \nfemale                                                                -0.03           \n                                                                     (0.07)           \n                                                                                      \npoc                                                                  -0.14*           \n                                                                     (0.08)           \n                                                                                      \nage_group                                                            0.22***          \n                                                                     (0.03)           \n                                                                                      \nwork_typeOn-call, work only when called to work                       -0.18           \n                                                                     (0.24)           \n                                                                                      \nwork_typePaid by a temporary agency                                 -1.62***          \n                                                                     (0.36)           \n                                                                                      \nwork_typeWork for contractor who provides workers/services            -0.35           \n                                                                     (0.22)           \n                                                                                      \nwork_typeRegular, permanent employee                                -0.55***          \n                                                                     (0.11)           \n                                                                                      \nyears_job                                                            -0.10**          \n                                                                     (0.04)           \n                                                                                      \n--------------------------------------------------------------------------------------\nObservations                                                          3,268           \n======================================================================================\nNote:                                                      *p<0.1; **p<0.05; ***p<0.01\n\n\nThe non linear relationship suggests that wrkhome should be coded as a factor."
  },
  {
    "objectID": "NJ_Adv_Quant.html#hypotheses",
    "href": "NJ_Adv_Quant.html#hypotheses",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Hypotheses",
    "text": "Hypotheses\n1: People who work mainly from home are more satisfied with their job than those who never work from home. 2: Working from home became more influential at making people more satisfied with their job post covid than pre covid. 3: In 2022, the working population is more satisfied with their jobs than they were in 2018. 4: People who are using technology for more than 75% of their total work time during a week are less satisfied with their jobs than others. 5: In post COVID times, using technology is related to lower job satisfaction than pre-COVID."
  },
  {
    "objectID": "NJ_Adv_Quant.html#final-models",
    "href": "NJ_Adv_Quant.html#final-models",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Final Models",
    "text": "Final Models\nHere I change to weights to type as.double() and run my final models.\n\n\nCode\nGSS$weights <- as.double(GSS$weights)\n\njs_l <- polr(job_sat ~ use_tech_f + wrkhome_f + post_covid  + female + poc + age_group + work_type + years_job +degree, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\nCode\njs_l_int<-polr(job_sat ~ use_tech_f + wrkhome_f + post_covid +use_tech_f:post_covid + wrkhome_f:post_covid + female + poc + age_group + work_type + years_job + degree, data=GSS, na.action = na.exclude, method = \"logistic\", weights= weights )\n\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!"
  },
  {
    "objectID": "NJ_Adv_Quant.html#display-results",
    "href": "NJ_Adv_Quant.html#display-results",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Display results",
    "text": "Display results\nHere I display my results of my main effect model and interaction effect model side by side using msummary(). I renamed all the coefficients to the correct labels. I include stars = TRUE to display the significance level of each variable.\n\n\nCode\nmsummary(list(\"Main Effect\" = js_l,\n              \"Interaction\" = js_l_int),\n         coef_rename = c(\"use_tech_f2\" = \"Uses tech 26-50% per week\",\n                          \"use_tech_f3\" = \"Uses tech 51-75% per week\",\n                          \"use_tech_f4\" = \"Uses tech >75% per week\",\n                          \"wrkhome_f2\"  = \"WFH few times a year\",\n                          \"wrkhome_f3\" = \"WFH about once a month\",\n                          \"wrkhome_f4\" = \"WFH about once a week\",\n                          \"wrkhome_f5\" = \"WFH more than once a week\",\n                          \"wrkhome_f6\" = \"Mainly WFH\",\n                          \"post_covid1\" =  \"2022\",\n                          \"female\" = \"Female\",\n                          \"poc\" =  \"POC\",\n                           \"age_group\" =  \"Age group\",\n                          \"work_typeOn-call, work only when called to work\"  = \"On-call, work only when called to work\",\n                          \"work_typePaid by a temporary agency\" =  \"Paid by a temporary agency\",\n                          \"work_typeWork for contractor who provides workers/services\" =\n                           \"Work for contractor who provides workers/services\",\n                          \"work_typeRegular, permanent employee\"  = \"Regular, permanent employee\",\n                           \"years_job\" = \"Years worked at current job\",\n                            \"degree\" = \"Education Level\",\n                           \"use_tech_f2 × post_covid1\"  = \"Uses tech 26-50% per week X 2022\",\n                           \"use_tech_f3 × post_covid1\" =   \"Uses tech 51-75% per week X 2022\",\n                           \"use_tech_f4 × post_covid1\" =  \"Uses tech >75% per week X 2022\",\n                           \"wrkhome_f2 × post_covid1\"  = \"WFH few times a year X 2022\",\n                            \"wrkhome_f3 × post_covid1\" =  \"WFH about once a month X 2022\",\n                            \"wrkhome_f4 × post_covid1\" = \"WFH about once a week X 2022\",\n                            \"wrkhome_f5 × post_covid\" = \"WFH more than once a week X 2022\",\n                            \"wrkhome_f6 × post_covid\" = \"Mainly WFH X 2022\"), stars = TRUE)\n\n\n\n\n \n  \n      \n    Main Effect \n     Interaction \n  \n \n\n  \n    1|2 \n    −4.080*** \n    −4.031*** \n  \n  \n     \n    (0.201) \n    (0.212) \n  \n  \n    2|3 \n    −2.694*** \n    −2.645*** \n  \n  \n     \n    (0.177) \n    (0.190) \n  \n  \n    3|4 \n    −0.291+ \n    −0.239 \n  \n  \n     \n    (0.169) \n    (0.183) \n  \n  \n    Uses tech 26-50% per week \n    −0.169 \n    −0.235 \n  \n  \n     \n    (0.105) \n    (0.156) \n  \n  \n    Uses tech 51-75% per week \n    0.035 \n    0.062 \n  \n  \n     \n    (0.127) \n    (0.186) \n  \n  \n    Uses tech >75% per week \n    −0.350*** \n    −0.241+ \n  \n  \n     \n    (0.091) \n    (0.131) \n  \n  \n    WFH few times a year \n    0.158 \n    0.167 \n  \n  \n     \n    (0.146) \n    (0.213) \n  \n  \n    WFH about once a month \n    0.584*** \n    0.470* \n  \n  \n     \n    (0.167) \n    (0.222) \n  \n  \n    WFH about once a week \n    0.297* \n    0.464* \n  \n  \n     \n    (0.139) \n    (0.192) \n  \n  \n    WFH more than once a week \n    0.291* \n    0.403* \n  \n  \n     \n    (0.114) \n    (0.175) \n  \n  \n    Mainly WFH \n    0.390** \n    0.328 \n  \n  \n     \n    (0.123) \n    (0.230) \n  \n  \n    2022 \n    −0.050 \n    0.035 \n  \n  \n     \n    (0.070) \n    (0.131) \n  \n  \n    Female \n    −0.029 \n    −0.027 \n  \n  \n     \n    (0.070) \n    (0.070) \n  \n  \n    POC \n    −0.144+ \n    −0.145+ \n  \n  \n     \n    (0.079) \n    (0.079) \n  \n  \n    Age group \n    0.223*** \n    0.225*** \n  \n  \n     \n    (0.032) \n    (0.032) \n  \n  \n    On-call, work only when called to work \n    −0.191 \n    −0.188 \n  \n  \n     \n    (0.243) \n    (0.244) \n  \n  \n    Paid by a temporary agency \n    −1.618*** \n    −1.628*** \n  \n  \n     \n    (0.364) \n    (0.365) \n  \n  \n    Work for contractor who provides workers/services \n    −0.343 \n    −0.347 \n  \n  \n     \n    (0.221) \n    (0.222) \n  \n  \n    Regular, permanent employee \n    −0.533*** \n    −0.529*** \n  \n  \n     \n    (0.112) \n    (0.113) \n  \n  \n    Years worked at current job \n    −0.094* \n    −0.098* \n  \n  \n     \n    (0.044) \n    (0.044) \n  \n  \n    Education Level \n    −0.052+ \n    −0.053+ \n  \n  \n     \n    (0.032) \n    (0.032) \n  \n  \n    Uses tech 26-50% per week:2022 \n     \n    0.124 \n  \n  \n     \n     \n    (0.209) \n  \n  \n    Uses tech 51-75% per week:2022 \n     \n    −0.027 \n  \n  \n     \n     \n    (0.250) \n  \n  \n    Uses tech >75% per week:2022 \n     \n    −0.191 \n  \n  \n     \n     \n    (0.175) \n  \n  \n    WFH few times a year:2022 \n     \n    −0.027 \n  \n  \n     \n     \n    (0.289) \n  \n  \n    WFH about once a month:2022 \n     \n    0.224 \n  \n  \n     \n     \n    (0.333) \n  \n  \n    WFH about once a week:2022 \n     \n    −0.353 \n  \n  \n     \n     \n    (0.272) \n  \n  \n    WFH more than once a week:2022 \n     \n    −0.167 \n  \n  \n     \n     \n    (0.225) \n  \n  \n    Mainly WFH:2022 \n     \n    0.109 \n  \n  \n     \n     \n    (0.266) \n  \n  \n    Num.Obs. \n    3268 \n    3268 \n  \n  \n    AIC \n    6323.9 \n    6334.1 \n  \n  \n    BIC \n    6451.8 \n    6510.8 \n  \n  \n    RMSE \n    3.20 \n    3.20 \n  \n\n\n + p < 0.1, * p < 0.05, ** p < 0.01, *** p < 0.001\n\n\n\n\nMain Effect model: Based on the model results, there is evidence that supports Hypothesis 1. Survey respondents who WFH about once a month (0.584 log odds) , WFH about once a week (0.297 log odds), WFH more than once a week (0.291 log odds), and Mainly WFH (0.390 log odds) are all significantly more likely (p < 0.05) to be satisfied with their jobs than survey respondents who never WFH. Additionally, there is evidence from the model results that supports Hypothesis 4. Survey respondents who use technology more than 75% of their total work time per week (−0.350 log odds) are significantly less likely (p <0.05) to be satisfied with their jobs than those who use technology less than 25% of their total work time per week. Hypothesis 3 is not supported based on the model results as ‘2022’ is not statistically different from 0 (−0.050 log odds , 0.70 SE). Interestingly, survey respondents who are Paid by a temporary agency (−0.533 log odds) or Regular, permanent employees (−1.618 log odds) are significantly less likely to be satisfied with their jobs than those who are Independent contractors. Other control variables that are statistically different from 0 at p < 0.05 include age group and Years worked at current job.\nInteraction model: Unfortunately, there is insufficient evidence that supports Hypotheses #2 and #5 as none of the interactions between working from home, the % of time using technology in a work week, and year (pre and post covid) are statistically different from 0 at p < 0.05."
  },
  {
    "objectID": "NJ_Adv_Quant.html#create-predicted-probabilites-for-use_tech",
    "href": "NJ_Adv_Quant.html#create-predicted-probabilites-for-use_tech",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Create predicted probabilites for use_tech",
    "text": "Create predicted probabilites for use_tech\n\n\nCode\nlogit<-ggpredict(js_l, terms=\"use_tech_f\")"
  },
  {
    "objectID": "NJ_Adv_Quant.html#graph-predicted-probabilities-of-use_tech",
    "href": "NJ_Adv_Quant.html#graph-predicted-probabilities-of-use_tech",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Graph Predicted probabilities of use_tech",
    "text": "Graph Predicted probabilities of use_tech\nI filtered out all non significant levels of use_tech. I also display the legend at the bottom of the graph for spacing purposes.\n\n\nCode\ncustom_levels <- c(1, 2, 3, 4)\ncustom_labels <- c(\"Not at all satisfied\", \"Not too satisfied\", \"Somewhat satisfied\", \"Very satisfied\")\n\n# Reassign variable with custom labels\nlogit$response.level <- factor(logit$response.level, levels = custom_levels, labels = custom_labels)  \n\n  \nlogit1 <- logit %>%\n    filter(logit$x %in% c(1, 4))\n#Filters for the variables we nee\n  \ncustom_order1 <- c('<=25% per week','>75% per week')  \n\nggplot(logit1, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n     geom_point() +   theme_minimal(base_size = 10) +\n    labs(x = \"% of tech used per week\", y = \"Predicted Probability\", \n         title = \"Predicted Probability of Job Satisfaction with 95% CI\") +\n    labs(color = \"Job Satisfaction\") +\n    geom_errorbar(aes(ymin=conf.low, ymax=conf.high),\n                  linewidth=.3,    # Thinner lines\n                  width=.2)  + scale_x_discrete(labels = custom_order1)  + theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 1: Predicted probabilities of the reference and significant levels of % of tech used per week during work\nFigure 1 shows that survey respondents who use technology more than 75% of their total work time per week have a lower predicted probability of being very satisfied with their job (~55% probability of being very satisfied) than those who use technology less than 25% of their total work time per week (~ 62% probability of being very satisfied). The reason for this could be similar to the conclusion found in the literature review about technology: the increased amount of information workers have to deal with due to using more technology is much harder to handle than workers who do not use as much technology in the workplace."
  },
  {
    "objectID": "NJ_Adv_Quant.html#graph-predicted-probabilities-of-wrkhome",
    "href": "NJ_Adv_Quant.html#graph-predicted-probabilities-of-wrkhome",
    "title": "U.S. Job Satisfaction: Impacts of Tech, Remote Work & Covid",
    "section": "Graph Predicted probabilities of wrkhome",
    "text": "Graph Predicted probabilities of wrkhome\nI filtered out all non significant levels of wrkhome. I also display the legend at the bottom of the graph for spacing purposes.\n\n\nCode\nlogitW<-ggpredict(js_l, terms=\"wrkhome_f\")\n\nlogitW$response.level <- factor(logitW$response.level, levels = custom_levels, labels = custom_labels)\nlogitW1 <- logitW %>%\n    filter(logitW$x %in% c(1,3,4,5,6))\n\ncustom_order2 <- c('Never WFH','WFH about \\n once a month', 'WFH about \\n once a week',\n                   'WFH more \\n than once a week', 'Mainly WFH' )  \n\nggplot(logitW1, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n     geom_point() +   theme_minimal(base_size = 10) +\n    labs(x = \"How Often Responents Work from Home \", y = \"Predicted Probability\", \n         title = \"Predicted Probability of Job Satisfaction with 95% CI\") +\n    labs(color = \"Job Satisfaction\") +\n    geom_errorbar(aes(ymin=conf.low, ymax=conf.high),\n                  linewidth=.3,    # Thinner lines\n                  width=.2)  + scale_x_discrete(labels = custom_order2)  + theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 2: Predicted probabilities of the reference and significant levels of how often respondents work from home\nFigure 2 shows that the survey respondents who Never WFH have a lower predicted probability of being very satisfied with their job (~ 60% probability of being very satisfied) than those who mainly WFH (~70% probability of being very satisfied). The reason for why this is the case could be that workers feel more comfortable in an at home work environment and favor the flexibility of working from home over working in person.\n\n\nCode\nlogitWT<-ggpredict(js_l, terms=\"work_type\")\n\nlogitWT$response.level <- factor(logitWT$response.level, levels = custom_levels, labels = custom_labels) \n\nlogitWT2 <- logitWT %>%\n    filter(logitWT$x %in% c('Independent contractor/consultant/freelance worker','Paid by a temporary agency','Regular, permanent employee'))\n\n#custom_order3 <- c('Independent contractor \\n /consultant/freelance \\n worker',\n#                   'On-call, \\n work only \\n when called to work','Paid by \\n a temporary \\n agency',\n#                   'Work for contractor \\n who provides \\n workers/services',\n#                   'Regular, \\n permanent employee' )\n\ncustom_order3 <- c('Independent contractor \\n /consultant/freelance \\n worker',\n                   'Paid by \\n a temporary \\n agency',\n                   'Regular, \\n permanent employee' )\n\nggplot(logitWT2, aes(x = x, y = predicted, color = response.level, group = response.level)) +\n     geom_point() +   theme_minimal(base_size = 10) +\n    labs(x = \"Work Type of Respondent \", y = \"Predicted Probability\", \n         title = \"Predicted Probability of Job Satisfaction with 95% CI\") +\n    labs(color = \"Job Satisfaction\") +\n    geom_errorbar(aes(ymin=conf.low, ymax=conf.high),\n                  linewidth=.3,    # Thinner lines\n                  width=.2)  + scale_x_discrete(labels = custom_order3) + theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3: Predicted probabilities of the reference and significant levels of work type of respondent\nFigure 3 shows the survey respondents who are Independent contractor/consultant/ freelance workers have a much higher predicted probability of being very satisfied with their job (~62% probability of being very satisfied) than survey respondents who are Paid by a temporary agency (~25% probability of being very satisfied) and regular, permanent employees (~ 50% probability of being very satisfied). The reason Independent contractor/consultant/ freelance workers have a greater chance of being very satisfied with jobs could be because of how flexible their jobs are. Although their job security may not be as safe as other types of work arrangements, Independent contractor/consultant/ freelance workers are tasked with work that they excel and are interested in, which leads towards the belief that they are more satisfied with their jobs than the other significant work arrangements."
  }
]